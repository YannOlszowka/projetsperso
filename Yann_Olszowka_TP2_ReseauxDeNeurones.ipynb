{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e16e447",
   "metadata": {},
   "source": [
    "Définition de l’architecture du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c38a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.datasets import boston_housing\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Activation, Dense \n",
    "from keras import initializers \n",
    "from keras import regularizers \n",
    "from keras import constraints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080d977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_architecture():\n",
    "    m = models.Sequential()\n",
    "    m.add(Dense(64, input_shape=(13, ), activation='relu'))\n",
    "    m.compile(optimizer='rmsprop',loss='mse')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b4a2b",
   "metadata": {},
   "source": [
    "Question 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e88097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réalisé à l'aide du pseudo-code, je le laisse même si je ne crois pas que c'est ce qui était demandé\n",
    "# import random\n",
    "#def perceptron_une_couche_64(DE,De):\n",
    "#    W = np.random.randint(-100, 100, size=(14,65))\n",
    "#    W = W*0.01\n",
    "#    lr = 0.01\n",
    "#    OS = 0\n",
    "#    t = 1\n",
    "#    while (OS-t)**2 > 0.1 :\n",
    "#        L = 0\n",
    "#        while np.size(De) != 0 :\n",
    "#            ir = random.randint(0,np.size(De)-1)\n",
    "#            o = np.zeros(14)\n",
    "#            o[0] = 1\n",
    "#            for i in range(13):\n",
    "#                o[i+1] = DE[ir,i]\n",
    "#            t = De[ir-1]\n",
    "#            O = np.zeros(65)\n",
    "#            O[0] = 1\n",
    "#            for j in range(1,65):\n",
    "#                for k in range(14):\n",
    "#                    O[j] = O[j] + o[k]*W[k,j]\n",
    "#                if O[j] < 0:\n",
    "#                    O[j] = 0\n",
    "#                else :\n",
    "#                    O[j] = O[j]\n",
    "#            OS = np.sum(O)\n",
    "#            L = L + (1/2)*((OS-t)**2)\n",
    "#            DELTAS = OS-t\n",
    "#            DELTA = np.zeros(65)\n",
    "#            for j in range(65):\n",
    "#                if OS >= 0:\n",
    "#                    DELTA[j] = DELTAS\n",
    "#                else:\n",
    "#                    DELTA[j] = 0\n",
    "#            delta = np.zeros(14)\n",
    "#            for i in range(14):\n",
    "#                for k in range(65):\n",
    "#                    if O[j] >= 0:\n",
    "#                        delta[i] = delta[i] + DELTA[k]*W[i,k]\n",
    "#                    else:\n",
    "#                        delta[i] = 0\n",
    "#            for i in range(14):\n",
    "#                for j in range(65):\n",
    "#                    W[i,j] = W[i,j] + lr*DELTA[j]*o[i]\n",
    "#            np.delete(De, ir)\n",
    "#            np.delete(DE, ir)\n",
    "#    return OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c0b15",
   "metadata": {},
   "source": [
    "Question 5)\n",
    "\n",
    "Sur la première couche on a un paramètre pour chaque connexion neuronale aux neurones de la couche cachée, c'est le poids de chaque connexion neuronale. Comme on a 13 entrées + 1 neurone de biais, on a 14*64 = 896 paramètres sur la première couche.\n",
    "\n",
    "Sur la couche cachée on a deux paramètres pour chaque connexion neuronale impliquant la couche de sortie. On oublie pas le neurone de biais de la couche cachée, et ainsi on a 2*(64+1) = 2*65 = 130 paramètres pour la couche cachée.\n",
    "\n",
    "Sur la couche de sortie on seulement 1 paramètre, c'est la valeur cible des estimations.\n",
    "\n",
    "Au total le réseau neuronal est à 896 + 130 + 1 = 1027 paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595e8e3",
   "metadata": {},
   "source": [
    "Apprentissage du réseau avec validation. Retourne le modèle ainsi que les valeurs de la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8e13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apprentissage(DA,Da):\n",
    "    m = def_architecture()\n",
    "    h = m.fit(DA,Da,validation_data=(DV,Dv),epochs=nb_époques,batch_size=1)\n",
    "    return m , h.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aab1d8",
   "metadata": {},
   "source": [
    "Chargement des bibliothèques et des modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e7633",
   "metadata": {},
   "source": [
    "Chargement de la base de données Boston Housing Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f211ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DA,Da),(DT,Dt) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059a5f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Da[104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef8e86",
   "metadata": {},
   "source": [
    "question 2) Ici, on centre et on normalise les données dans le but de les homogénéiser, ainsi chaque couple de données appartenant à (DA,Da) a la même importance relativement aux autres couples impliqués dans le calcul de la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b591beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DA.mean(axis=0); s = DA.std(axis=0); DA-=m ; DA/=s; DT-=m; DT/=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a5b2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_époques=300\n",
    "N=DA.shape[0]; NV=N//4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f19d1",
   "metadata": {},
   "source": [
    "question 3) Ici, on partitionne notre ensemble d'entrainement en un ensemble de validation avec le premier quart de l'ensemble des éléments de l'ensemble d'entrainement, et en un autre ensemble composé du reste des éléments de l'ensemble d'entrainement.\n",
    "Le but de la démarche est de continuer à améliorer l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c5fc1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "303/303 [==============================] - 1s 1ms/step - loss: 569.9056 - val_loss: 541.2836\n",
      "Epoch 2/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 546.8922 - val_loss: 516.9024\n",
      "Epoch 3/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 523.0638 - val_loss: 492.9336\n",
      "Epoch 4/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 499.6652 - val_loss: 470.1991\n",
      "Epoch 5/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 477.1375 - val_loss: 448.3931\n",
      "Epoch 6/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 456.4127 - val_loss: 428.0652\n",
      "Epoch 7/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 436.1565 - val_loss: 408.7157\n",
      "Epoch 8/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 417.4586 - val_loss: 390.7630\n",
      "Epoch 9/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 399.1582 - val_loss: 373.6704\n",
      "Epoch 10/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 382.2900 - val_loss: 357.9766\n",
      "Epoch 11/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 366.2350 - val_loss: 343.1629\n",
      "Epoch 12/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 351.1128 - val_loss: 329.5270\n",
      "Epoch 13/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 337.6784 - val_loss: 317.1748\n",
      "Epoch 14/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 324.5711 - val_loss: 305.5139\n",
      "Epoch 15/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 312.2270 - val_loss: 294.8647\n",
      "Epoch 16/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 300.8138 - val_loss: 285.1356\n",
      "Epoch 17/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 290.6475 - val_loss: 276.2987\n",
      "Epoch 18/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 280.9452 - val_loss: 268.2517\n",
      "Epoch 19/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 271.9674 - val_loss: 260.8012\n",
      "Epoch 20/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 263.8498 - val_loss: 253.8638\n",
      "Epoch 21/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 256.0371 - val_loss: 247.5801\n",
      "Epoch 22/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 248.6534 - val_loss: 241.7477\n",
      "Epoch 23/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 241.7397 - val_loss: 236.3005\n",
      "Epoch 24/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 235.4597 - val_loss: 231.3462\n",
      "Epoch 25/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 229.4715 - val_loss: 226.6435\n",
      "Epoch 26/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 223.8871 - val_loss: 222.3916\n",
      "Epoch 27/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 218.7069 - val_loss: 218.3640\n",
      "Epoch 28/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 213.7093 - val_loss: 214.6224\n",
      "Epoch 29/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 209.0911 - val_loss: 211.0812\n",
      "Epoch 30/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 204.7125 - val_loss: 207.7784\n",
      "Epoch 31/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 200.5502 - val_loss: 204.6518\n",
      "Epoch 32/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 196.4405 - val_loss: 201.6350\n",
      "Epoch 33/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 192.4592 - val_loss: 198.7926\n",
      "Epoch 34/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 188.7814 - val_loss: 196.0616\n",
      "Epoch 35/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 185.3265 - val_loss: 193.5258\n",
      "Epoch 36/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 181.9484 - val_loss: 191.0829\n",
      "Epoch 37/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 178.8164 - val_loss: 188.8133\n",
      "Epoch 38/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 175.7232 - val_loss: 186.6387\n",
      "Epoch 39/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 172.8372 - val_loss: 184.5053\n",
      "Epoch 40/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 169.9576 - val_loss: 182.4878\n",
      "Epoch 41/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 167.1960 - val_loss: 180.5339\n",
      "Epoch 42/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 164.5068 - val_loss: 178.6598\n",
      "Epoch 43/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 161.9431 - val_loss: 176.8842\n",
      "Epoch 44/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 159.4867 - val_loss: 175.1916\n",
      "Epoch 45/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 157.0745 - val_loss: 173.4974\n",
      "Epoch 46/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 154.7901 - val_loss: 171.8583\n",
      "Epoch 47/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 152.5231 - val_loss: 170.1921\n",
      "Epoch 48/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 150.3833 - val_loss: 168.5119\n",
      "Epoch 49/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 148.1984 - val_loss: 166.8580\n",
      "Epoch 50/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 146.2505 - val_loss: 165.2421\n",
      "Epoch 51/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 144.2434 - val_loss: 163.5708\n",
      "Epoch 52/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 142.3097 - val_loss: 161.8979\n",
      "Epoch 53/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 140.4732 - val_loss: 160.2187\n",
      "Epoch 54/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 138.7206 - val_loss: 158.6355\n",
      "Epoch 55/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 137.0083 - val_loss: 157.0004\n",
      "Epoch 56/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 135.2931 - val_loss: 155.3874\n",
      "Epoch 57/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 133.6981 - val_loss: 153.7605\n",
      "Epoch 58/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 132.0895 - val_loss: 152.1632\n",
      "Epoch 59/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 130.5753 - val_loss: 150.5613\n",
      "Epoch 60/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 129.0956 - val_loss: 148.9161\n",
      "Epoch 61/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 127.6404 - val_loss: 147.1268\n",
      "Epoch 62/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 126.2400 - val_loss: 145.3878\n",
      "Epoch 63/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 124.7915 - val_loss: 143.6507\n",
      "Epoch 64/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 123.4913 - val_loss: 141.9748\n",
      "Epoch 65/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 122.1577 - val_loss: 140.4253\n",
      "Epoch 66/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 120.8816 - val_loss: 138.8335\n",
      "Epoch 67/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 119.6862 - val_loss: 137.3013\n",
      "Epoch 68/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 118.4718 - val_loss: 135.9136\n",
      "Epoch 69/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 117.3858 - val_loss: 134.4764\n",
      "Epoch 70/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 116.3092 - val_loss: 133.1868\n",
      "Epoch 71/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 115.2758 - val_loss: 131.8471\n",
      "Epoch 72/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.2502 - val_loss: 130.5011\n",
      "Epoch 73/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.2683 - val_loss: 129.2113\n",
      "Epoch 74/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 112.3111 - val_loss: 127.9747\n",
      "Epoch 75/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.4322 - val_loss: 126.8518\n",
      "Epoch 76/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.5773 - val_loss: 125.7278\n",
      "Epoch 77/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.7140 - val_loss: 124.6334\n",
      "Epoch 78/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.9195 - val_loss: 123.5991\n",
      "Epoch 79/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.1951 - val_loss: 122.6513\n",
      "Epoch 80/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.4857 - val_loss: 121.7111\n",
      "Epoch 81/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.7808 - val_loss: 120.7561\n",
      "Epoch 82/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.0317 - val_loss: 119.8427\n",
      "Epoch 83/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.4195 - val_loss: 119.0419\n",
      "Epoch 84/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.7619 - val_loss: 118.2341\n",
      "Epoch 85/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.1852 - val_loss: 117.4534\n",
      "Epoch 86/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.6033 - val_loss: 116.7291\n",
      "Epoch 87/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.0350 - val_loss: 115.9992\n",
      "Epoch 88/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.5227 - val_loss: 115.3522\n",
      "Epoch 89/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.9866 - val_loss: 114.7037\n",
      "Epoch 90/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.4508 - val_loss: 114.0409\n",
      "Epoch 91/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.9614 - val_loss: 113.4360\n",
      "Epoch 92/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.4644 - val_loss: 112.8469\n",
      "Epoch 93/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.9747 - val_loss: 112.2597\n",
      "Epoch 94/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.5195 - val_loss: 111.6824\n",
      "Epoch 95/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.0566 - val_loss: 111.1357\n",
      "Epoch 96/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.5916 - val_loss: 110.5632\n",
      "Epoch 97/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.1454 - val_loss: 110.0219\n",
      "Epoch 98/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.7344 - val_loss: 109.5240\n",
      "Epoch 99/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.3054 - val_loss: 109.0245\n",
      "Epoch 100/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.9092 - val_loss: 108.5609\n",
      "Epoch 101/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.4928 - val_loss: 108.0637\n",
      "Epoch 102/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.0909 - val_loss: 107.5818\n",
      "Epoch 103/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.7267 - val_loss: 107.1478\n",
      "Epoch 104/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.3610 - val_loss: 106.7302\n",
      "Epoch 105/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.0347 - val_loss: 106.3026\n",
      "Epoch 106/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.6673 - val_loss: 105.8502\n",
      "Epoch 107/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.3402 - val_loss: 105.4564\n",
      "Epoch 108/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.0086 - val_loss: 105.0778\n",
      "Epoch 109/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.7170 - val_loss: 104.6937\n",
      "Epoch 110/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.3860 - val_loss: 104.3211\n",
      "Epoch 111/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.0938 - val_loss: 103.9668\n",
      "Epoch 112/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.7952 - val_loss: 103.6123\n",
      "Epoch 113/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.4923 - val_loss: 103.2641\n",
      "Epoch 114/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.2279 - val_loss: 102.9312\n",
      "Epoch 115/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.9441 - val_loss: 102.5873\n",
      "Epoch 116/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.6557 - val_loss: 102.2812\n",
      "Epoch 117/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.4170 - val_loss: 101.9888\n",
      "Epoch 118/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.1543 - val_loss: 101.7064\n",
      "Epoch 119/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.9283 - val_loss: 101.4085\n",
      "Epoch 120/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.6987 - val_loss: 101.1335\n",
      "Epoch 121/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.4734 - val_loss: 100.8400\n",
      "Epoch 122/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.2547 - val_loss: 100.5650\n",
      "Epoch 123/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.0342 - val_loss: 100.3195\n",
      "Epoch 124/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.8648 - val_loss: 100.0437\n",
      "Epoch 125/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.6258 - val_loss: 99.8162\n",
      "Epoch 126/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.4107 - val_loss: 99.5669\n",
      "Epoch 127/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.2218 - val_loss: 99.3305\n",
      "Epoch 128/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.0405 - val_loss: 99.0748\n",
      "Epoch 129/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.8480 - val_loss: 98.8578\n",
      "Epoch 130/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.6579 - val_loss: 98.6240\n",
      "Epoch 131/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.4899 - val_loss: 98.4094\n",
      "Epoch 132/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.3041 - val_loss: 98.2096\n",
      "Epoch 133/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.1131 - val_loss: 97.9719\n",
      "Epoch 134/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9418 - val_loss: 97.7587\n",
      "Epoch 135/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.7751 - val_loss: 97.5515\n",
      "Epoch 136/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.6156 - val_loss: 97.3275\n",
      "Epoch 137/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.4696 - val_loss: 97.1274\n",
      "Epoch 138/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.3139 - val_loss: 96.9433\n",
      "Epoch 139/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.1708 - val_loss: 96.7546\n",
      "Epoch 140/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.0164 - val_loss: 96.5473\n",
      "Epoch 141/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.8762 - val_loss: 96.3541\n",
      "Epoch 142/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.7473 - val_loss: 96.1527\n",
      "Epoch 143/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.6171 - val_loss: 95.9554\n",
      "Epoch 144/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.4707 - val_loss: 95.7430\n",
      "Epoch 145/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.3480 - val_loss: 95.5667\n",
      "Epoch 146/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.2072 - val_loss: 95.3520\n",
      "Epoch 147/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.0651 - val_loss: 95.1872\n",
      "Epoch 148/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.9359 - val_loss: 94.9657\n",
      "Epoch 149/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8240 - val_loss: 94.8369\n",
      "Epoch 150/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.7005 - val_loss: 94.6813\n",
      "Epoch 151/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.5848 - val_loss: 94.4997\n",
      "Epoch 152/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.4759 - val_loss: 94.3212\n",
      "Epoch 153/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.3591 - val_loss: 94.1639\n",
      "Epoch 154/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.2601 - val_loss: 93.9851\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 85.1512 - val_loss: 93.8192\n",
      "Epoch 156/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.0319 - val_loss: 93.6603\n",
      "Epoch 157/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.9247 - val_loss: 93.4906\n",
      "Epoch 158/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.8308 - val_loss: 93.3354\n",
      "Epoch 159/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.7152 - val_loss: 93.1486\n",
      "Epoch 160/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.6231 - val_loss: 92.9871\n",
      "Epoch 161/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.5201 - val_loss: 92.8348\n",
      "Epoch 162/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.4192 - val_loss: 92.6457\n",
      "Epoch 163/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.3137 - val_loss: 92.4931\n",
      "Epoch 164/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2030 - val_loss: 92.2894\n",
      "Epoch 165/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.1077 - val_loss: 92.1570\n",
      "Epoch 166/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.0114 - val_loss: 91.9519\n",
      "Epoch 167/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.9238 - val_loss: 91.7878\n",
      "Epoch 168/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.8363 - val_loss: 91.6519\n",
      "Epoch 169/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.7316 - val_loss: 91.5304\n",
      "Epoch 170/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6489 - val_loss: 91.3851\n",
      "Epoch 171/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.5728 - val_loss: 91.2213\n",
      "Epoch 172/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4809 - val_loss: 91.0495\n",
      "Epoch 173/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4054 - val_loss: 90.8914\n",
      "Epoch 174/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.3202 - val_loss: 90.7337\n",
      "Epoch 175/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.2192 - val_loss: 90.5694\n",
      "Epoch 176/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.1584 - val_loss: 90.4125\n",
      "Epoch 177/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.0484 - val_loss: 90.2151\n",
      "Epoch 178/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.9402 - val_loss: 90.0741\n",
      "Epoch 179/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8468 - val_loss: 89.9196\n",
      "Epoch 180/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7722 - val_loss: 89.8063\n",
      "Epoch 181/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.6898 - val_loss: 89.6387\n",
      "Epoch 182/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.6040 - val_loss: 89.4760\n",
      "Epoch 183/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.4893 - val_loss: 89.3208\n",
      "Epoch 184/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3874 - val_loss: 89.1658\n",
      "Epoch 185/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.2807 - val_loss: 89.0275\n",
      "Epoch 186/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.1850 - val_loss: 88.8786\n",
      "Epoch 187/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0790 - val_loss: 88.7292\n",
      "Epoch 188/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.9818 - val_loss: 88.5701\n",
      "Epoch 189/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.8850 - val_loss: 88.4209\n",
      "Epoch 190/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.7691 - val_loss: 88.2544\n",
      "Epoch 191/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6466 - val_loss: 88.1202\n",
      "Epoch 192/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.5368 - val_loss: 88.0232\n",
      "Epoch 193/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.4446 - val_loss: 87.8566\n",
      "Epoch 194/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.3407 - val_loss: 87.7265\n",
      "Epoch 195/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.2339 - val_loss: 87.5824\n",
      "Epoch 196/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0907 - val_loss: 87.4116\n",
      "Epoch 197/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.9579 - val_loss: 87.2491\n",
      "Epoch 198/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.8460 - val_loss: 87.0978\n",
      "Epoch 199/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.7192 - val_loss: 86.9795\n",
      "Epoch 200/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.6148 - val_loss: 86.8412\n",
      "Epoch 201/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.4811 - val_loss: 86.7131\n",
      "Epoch 202/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.3742 - val_loss: 86.5782\n",
      "Epoch 203/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.2531 - val_loss: 86.4269\n",
      "Epoch 204/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.1316 - val_loss: 86.2598\n",
      "Epoch 205/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.9901 - val_loss: 86.0990\n",
      "Epoch 206/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.8813 - val_loss: 85.9734\n",
      "Epoch 207/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.7678 - val_loss: 85.8025\n",
      "Epoch 208/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.6411 - val_loss: 85.6195\n",
      "Epoch 209/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.5315 - val_loss: 85.4549\n",
      "Epoch 210/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.4391 - val_loss: 85.3263\n",
      "Epoch 211/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3110 - val_loss: 85.1634\n",
      "Epoch 212/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 79.1930 - val_loss: 85.0098\n",
      "Epoch 213/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.0882 - val_loss: 84.8844\n",
      "Epoch 214/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.9929 - val_loss: 84.7424\n",
      "Epoch 215/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.8653 - val_loss: 84.5997\n",
      "Epoch 216/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.7597 - val_loss: 84.4465\n",
      "Epoch 217/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.6448 - val_loss: 84.3084\n",
      "Epoch 218/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.5568 - val_loss: 84.1586\n",
      "Epoch 219/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.4366 - val_loss: 84.0420\n",
      "Epoch 220/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.3509 - val_loss: 83.8916\n",
      "Epoch 221/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.2416 - val_loss: 83.7710\n",
      "Epoch 222/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.1360 - val_loss: 83.6541\n",
      "Epoch 223/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.0472 - val_loss: 83.5177\n",
      "Epoch 224/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.9609 - val_loss: 83.3735\n",
      "Epoch 225/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.8524 - val_loss: 83.2557\n",
      "Epoch 226/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.7821 - val_loss: 83.1086\n",
      "Epoch 227/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6851 - val_loss: 83.0287\n",
      "Epoch 228/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6088 - val_loss: 82.9249\n",
      "Epoch 229/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4848 - val_loss: 82.7685\n",
      "Epoch 230/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4069 - val_loss: 82.6540\n",
      "Epoch 231/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.3072 - val_loss: 82.5198\n",
      "Epoch 232/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.1979 - val_loss: 82.3633\n",
      "Epoch 233/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0985 - val_loss: 82.2491\n",
      "Epoch 234/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0068 - val_loss: 82.1217\n",
      "Epoch 235/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8911 - val_loss: 81.9991\n",
      "Epoch 236/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8060 - val_loss: 81.8888\n",
      "Epoch 237/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.7425 - val_loss: 81.8215\n",
      "Epoch 238/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.6404 - val_loss: 81.6991\n",
      "Epoch 239/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.5543 - val_loss: 81.5706\n",
      "Epoch 240/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4826 - val_loss: 81.4499\n",
      "Epoch 241/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4073 - val_loss: 81.3524\n",
      "Epoch 242/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.3271 - val_loss: 81.2691\n",
      "Epoch 243/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.2572 - val_loss: 81.1911\n",
      "Epoch 244/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.2111 - val_loss: 81.1249\n",
      "Epoch 245/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.1412 - val_loss: 81.0240\n",
      "Epoch 246/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0820 - val_loss: 80.9339\n",
      "Epoch 247/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0182 - val_loss: 80.7998\n",
      "Epoch 248/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.9416 - val_loss: 80.6822\n",
      "Epoch 249/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 75.8879 - val_loss: 80.5779\n",
      "Epoch 250/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8180 - val_loss: 80.4745\n",
      "Epoch 251/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.7558 - val_loss: 80.3939\n",
      "Epoch 252/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6800 - val_loss: 80.2686\n",
      "Epoch 253/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6114 - val_loss: 80.1641\n",
      "Epoch 254/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.5529 - val_loss: 80.0614\n",
      "Epoch 255/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4789 - val_loss: 79.9478\n",
      "Epoch 256/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4118 - val_loss: 79.8785\n",
      "Epoch 257/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3494 - val_loss: 79.8107\n",
      "Epoch 258/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.2804 - val_loss: 79.7080\n",
      "Epoch 259/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1889 - val_loss: 79.5784\n",
      "Epoch 260/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1253 - val_loss: 79.4490\n",
      "Epoch 261/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0579 - val_loss: 79.3437\n",
      "Epoch 262/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.9649 - val_loss: 79.2303\n",
      "Epoch 263/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8871 - val_loss: 79.1403\n",
      "Epoch 264/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8095 - val_loss: 79.0374\n",
      "Epoch 265/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.7308 - val_loss: 78.9148\n",
      "Epoch 266/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.6359 - val_loss: 78.7743\n",
      "Epoch 267/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.5732 - val_loss: 78.6710\n",
      "Epoch 268/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.4631 - val_loss: 78.5794\n",
      "Epoch 269/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.3923 - val_loss: 78.4333\n",
      "Epoch 270/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.3156 - val_loss: 78.2909\n",
      "Epoch 271/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.2335 - val_loss: 78.1388\n",
      "Epoch 272/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.1520 - val_loss: 78.0340\n",
      "Epoch 273/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.0433 - val_loss: 77.8972\n",
      "Epoch 274/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.9752 - val_loss: 77.7795\n",
      "Epoch 275/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.8921 - val_loss: 77.6717\n",
      "Epoch 276/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.8091 - val_loss: 77.5464\n",
      "Epoch 277/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7162 - val_loss: 77.4154\n",
      "Epoch 278/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.6171 - val_loss: 77.2686\n",
      "Epoch 279/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.5076 - val_loss: 77.1423\n",
      "Epoch 280/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.4335 - val_loss: 77.0586\n",
      "Epoch 281/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.3428 - val_loss: 76.8977\n",
      "Epoch 282/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.2562 - val_loss: 76.7449\n",
      "Epoch 283/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.1292 - val_loss: 76.5700\n",
      "Epoch 284/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.0031 - val_loss: 76.4290\n",
      "Epoch 285/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.8733 - val_loss: 76.2439\n",
      "Epoch 286/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.7145 - val_loss: 76.0505\n",
      "Epoch 287/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.5620 - val_loss: 75.8624\n",
      "Epoch 288/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.4243 - val_loss: 75.6861\n",
      "Epoch 289/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.2930 - val_loss: 75.4784\n",
      "Epoch 290/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.1123 - val_loss: 75.2229\n",
      "Epoch 291/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.9213 - val_loss: 74.9491\n",
      "Epoch 292/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.7613 - val_loss: 74.7204\n",
      "Epoch 293/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.5290 - val_loss: 74.3946\n",
      "Epoch 294/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.3294 - val_loss: 74.1205\n",
      "Epoch 295/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.1027 - val_loss: 73.8502\n",
      "Epoch 296/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.8815 - val_loss: 73.5691\n",
      "Epoch 297/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 70.5943 - val_loss: 73.2746\n",
      "Epoch 298/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.3635 - val_loss: 72.9829\n",
      "Epoch 299/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.0616 - val_loss: 72.6418\n",
      "Epoch 300/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.7540 - val_loss: 72.2656\n"
     ]
    }
   ],
   "source": [
    "DV=DA[:NV]; Dv=Da[:NV]; DAp=DA[NV:]; Dap=Da[NV:]\n",
    "modèle,h=Apprentissage(DAp,Dap)\n",
    "DA_L=h['loss']; DV_L=h['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb9778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = [i for i in range(10,300)]\n",
    "DA_L10 = DA_L[10:]\n",
    "DV_L10 = DV_L[10:]\n",
    "plt.plot(niter, DA_L10)\n",
    "plt.xlabel('itération')\n",
    "plt.ylabel('erreur d entrainement')\n",
    "plt.plot(niter, DV_L10)\n",
    "plt.xlabel('itération')\n",
    "plt.ylabel('erreur de validation')\n",
    "plt.title('Les erreurs en moyenne quadratique selon la quantite d iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98e7b3",
   "metadata": {},
   "source": [
    "L'erreur de validation est plus petite que l'erreur d'entrainement pour les premières époques d'apprentissage, la valeur absolue de la différence entre les deux erreurs diminue jusqu'à ce que l'erreur de validation soi plus grande que l'erreur d'entrainement pour l'époque 28 d'apprentissage, suite à ça les erreurs continuent à décroître et l'erreur de validation reste plus grande que l'erreur d'entrainement.\n",
    "La décroissance est de moins en moins fortes au fur et à mesure qu'on avance dans les époques d'apprentissage, et cela pour les deux erreurs.\n",
    "Ainsi les erreurs sont minimales pour l'époque la plus grande, c'est à dire la 300ième."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9b389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "303/303 [==============================] - 1s 1ms/step - loss: 570.8166 - val_loss: 543.5945\n",
      "Epoch 2/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 548.5607 - val_loss: 519.5830\n",
      "Epoch 3/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 524.8167 - val_loss: 495.8115\n",
      "Epoch 4/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 501.2871 - val_loss: 472.3738\n",
      "Epoch 5/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 478.0639 - val_loss: 449.7850\n",
      "Epoch 6/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 456.2315 - val_loss: 428.2119\n",
      "Epoch 7/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 434.9128 - val_loss: 407.3967\n",
      "Epoch 8/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 414.0827 - val_loss: 387.8369\n",
      "Epoch 9/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 394.7997 - val_loss: 369.4562\n",
      "Epoch 10/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 377.1580 - val_loss: 352.4247\n",
      "Epoch 11/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 359.8718 - val_loss: 336.7613\n",
      "Epoch 12/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 344.0026 - val_loss: 322.2313\n",
      "Epoch 13/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 329.2270 - val_loss: 308.6458\n",
      "Epoch 14/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 315.4596 - val_loss: 296.0446\n",
      "Epoch 15/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 302.3297 - val_loss: 284.6472\n",
      "Epoch 16/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 290.7114 - val_loss: 274.2451\n",
      "Epoch 17/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 279.9984 - val_loss: 264.9148\n",
      "Epoch 18/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 270.2697 - val_loss: 256.2986\n",
      "Epoch 19/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 260.5345 - val_loss: 248.2537\n",
      "Epoch 20/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 252.4350 - val_loss: 241.2985\n",
      "Epoch 21/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 244.6241 - val_loss: 234.7516\n",
      "Epoch 22/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 237.3173 - val_loss: 228.7389\n",
      "Epoch 23/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 230.1546 - val_loss: 223.3768\n",
      "Epoch 24/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 224.3350 - val_loss: 218.5217\n",
      "Epoch 25/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 218.2558 - val_loss: 214.0002\n",
      "Epoch 26/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 212.7658 - val_loss: 209.9624\n",
      "Epoch 27/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 207.7672 - val_loss: 206.2538\n",
      "Epoch 28/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 203.1580 - val_loss: 202.7853\n",
      "Epoch 29/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 198.6594 - val_loss: 199.5274\n",
      "Epoch 30/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 194.4795 - val_loss: 196.4999\n",
      "Epoch 31/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 190.3979 - val_loss: 193.6366\n",
      "Epoch 32/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 186.6256 - val_loss: 190.9645\n",
      "Epoch 33/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 183.0930 - val_loss: 188.4700\n",
      "Epoch 34/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 179.6135 - val_loss: 186.1271\n",
      "Epoch 35/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 176.4115 - val_loss: 183.8971\n",
      "Epoch 36/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 173.2657 - val_loss: 181.7225\n",
      "Epoch 37/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 170.3419 - val_loss: 179.6498\n",
      "Epoch 38/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 167.4562 - val_loss: 177.6006\n",
      "Epoch 39/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 164.5892 - val_loss: 175.6586\n",
      "Epoch 40/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 161.9619 - val_loss: 173.7921\n",
      "Epoch 41/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 159.4234 - val_loss: 171.9598\n",
      "Epoch 42/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 156.9765 - val_loss: 170.1982\n",
      "Epoch 43/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 154.5658 - val_loss: 168.4470\n",
      "Epoch 44/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 152.3090 - val_loss: 166.7231\n",
      "Epoch 45/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 150.0399 - val_loss: 165.0398\n",
      "Epoch 46/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 147.9830 - val_loss: 163.3733\n",
      "Epoch 47/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 145.8958 - val_loss: 161.7251\n",
      "Epoch 48/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 143.9481 - val_loss: 160.1394\n",
      "Epoch 49/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 141.9721 - val_loss: 158.5761\n",
      "Epoch 50/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 140.1727 - val_loss: 156.9743\n",
      "Epoch 51/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 138.3800 - val_loss: 155.4234\n",
      "Epoch 52/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 136.7431 - val_loss: 153.9099\n",
      "Epoch 53/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 135.1259 - val_loss: 152.4318\n",
      "Epoch 54/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 133.5717 - val_loss: 150.9687\n",
      "Epoch 55/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 132.0378 - val_loss: 149.4439\n",
      "Epoch 56/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 130.5802 - val_loss: 147.9682\n",
      "Epoch 57/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 129.1937 - val_loss: 146.5154\n",
      "Epoch 58/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 127.8001 - val_loss: 145.0345\n",
      "Epoch 59/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 126.4631 - val_loss: 143.5353\n",
      "Epoch 60/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 125.1538 - val_loss: 142.0555\n",
      "Epoch 61/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 123.8778 - val_loss: 140.6259\n",
      "Epoch 62/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 122.7433 - val_loss: 139.2715\n",
      "Epoch 63/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 121.5643 - val_loss: 137.8659\n",
      "Epoch 64/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 120.4757 - val_loss: 136.5785\n",
      "Epoch 65/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 119.3923 - val_loss: 135.3198\n",
      "Epoch 66/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 118.3865 - val_loss: 134.0982\n",
      "Epoch 67/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 117.4016 - val_loss: 132.9090\n",
      "Epoch 68/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 116.4562 - val_loss: 131.7618\n",
      "Epoch 69/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 115.6084 - val_loss: 130.6533\n",
      "Epoch 70/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.7197 - val_loss: 129.5861\n",
      "Epoch 71/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.9273 - val_loss: 128.6185\n",
      "Epoch 72/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.1457 - val_loss: 127.6585\n",
      "Epoch 73/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 112.3906 - val_loss: 126.7149\n",
      "Epoch 74/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.6941 - val_loss: 125.8536\n",
      "Epoch 75/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.0251 - val_loss: 124.9749\n",
      "Epoch 76/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.3603 - val_loss: 124.1495\n",
      "Epoch 77/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.6807 - val_loss: 123.3211\n",
      "Epoch 78/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.0862 - val_loss: 122.5350\n",
      "Epoch 79/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.4723 - val_loss: 121.8075\n",
      "Epoch 80/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.8926 - val_loss: 121.1000\n",
      "Epoch 81/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.3538 - val_loss: 120.4225\n",
      "Epoch 82/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.8323 - val_loss: 119.7804\n",
      "Epoch 83/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.3455 - val_loss: 119.1197\n",
      "Epoch 84/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.7938 - val_loss: 118.4862\n",
      "Epoch 85/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.2716 - val_loss: 117.8823\n",
      "Epoch 86/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.8243 - val_loss: 117.2944\n",
      "Epoch 87/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.3699 - val_loss: 116.7122\n",
      "Epoch 88/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.9031 - val_loss: 116.1480\n",
      "Epoch 89/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.4445 - val_loss: 115.6024\n",
      "Epoch 90/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.9899 - val_loss: 115.0540\n",
      "Epoch 91/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.5732 - val_loss: 114.5505\n",
      "Epoch 92/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.1527 - val_loss: 114.0457\n",
      "Epoch 93/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.7421 - val_loss: 113.5508\n",
      "Epoch 94/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.3542 - val_loss: 113.0561\n",
      "Epoch 95/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.9699 - val_loss: 112.6003\n",
      "Epoch 96/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.6397 - val_loss: 112.1743\n",
      "Epoch 97/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.2786 - val_loss: 111.7687\n",
      "Epoch 98/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.9258 - val_loss: 111.3492\n",
      "Epoch 99/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.5971 - val_loss: 110.9152\n",
      "Epoch 100/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.2371 - val_loss: 110.4942\n",
      "Epoch 101/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.9189 - val_loss: 110.0858\n",
      "Epoch 102/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.5660 - val_loss: 109.6989\n",
      "Epoch 103/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.2521 - val_loss: 109.3321\n",
      "Epoch 104/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.9587 - val_loss: 108.9432\n",
      "Epoch 105/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.6407 - val_loss: 108.5649\n",
      "Epoch 106/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.3573 - val_loss: 108.2215\n",
      "Epoch 107/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.0733 - val_loss: 107.8904\n",
      "Epoch 108/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.7957 - val_loss: 107.5426\n",
      "Epoch 109/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.5016 - val_loss: 107.2222\n",
      "Epoch 110/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.2522 - val_loss: 106.9311\n",
      "Epoch 111/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.0089 - val_loss: 106.6099\n",
      "Epoch 112/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.7400 - val_loss: 106.3005\n",
      "Epoch 113/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.5040 - val_loss: 106.0166\n",
      "Epoch 114/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.2747 - val_loss: 105.7301\n",
      "Epoch 115/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.0377 - val_loss: 105.4666\n",
      "Epoch 116/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.8072 - val_loss: 105.1952\n",
      "Epoch 117/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.5912 - val_loss: 104.9384\n",
      "Epoch 118/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.3859 - val_loss: 104.6747\n",
      "Epoch 119/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.1703 - val_loss: 104.4213\n",
      "Epoch 120/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.9420 - val_loss: 104.1579\n",
      "Epoch 121/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.7384 - val_loss: 103.9215\n",
      "Epoch 122/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.5542 - val_loss: 103.6988\n",
      "Epoch 123/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.3508 - val_loss: 103.4762\n",
      "Epoch 124/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.1571 - val_loss: 103.2413\n",
      "Epoch 125/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.9795 - val_loss: 103.0206\n",
      "Epoch 126/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.8049 - val_loss: 102.7966\n",
      "Epoch 127/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.6242 - val_loss: 102.5831\n",
      "Epoch 128/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.4426 - val_loss: 102.3845\n",
      "Epoch 129/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.2821 - val_loss: 102.1951\n",
      "Epoch 130/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.1210 - val_loss: 101.9985\n",
      "Epoch 131/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.9756 - val_loss: 101.7716\n",
      "Epoch 132/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.8008 - val_loss: 101.5439\n",
      "Epoch 133/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.6355 - val_loss: 101.3334\n",
      "Epoch 134/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.4591 - val_loss: 101.1555\n",
      "Epoch 135/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.3111 - val_loss: 100.9565\n",
      "Epoch 136/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.1468 - val_loss: 100.7617\n",
      "Epoch 137/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.0206 - val_loss: 100.5440\n",
      "Epoch 138/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.8671 - val_loss: 100.3435\n",
      "Epoch 139/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.7345 - val_loss: 100.1118\n",
      "Epoch 140/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.5907 - val_loss: 99.9369\n",
      "Epoch 141/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.4596 - val_loss: 99.7529\n",
      "Epoch 142/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.3189 - val_loss: 99.5646\n",
      "Epoch 143/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.2133 - val_loss: 99.3823\n",
      "Epoch 144/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.0861 - val_loss: 99.1956\n",
      "Epoch 145/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.9661 - val_loss: 99.0055\n",
      "Epoch 146/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.8625 - val_loss: 98.8442\n",
      "Epoch 147/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.7342 - val_loss: 98.6802\n",
      "Epoch 148/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.6251 - val_loss: 98.5253\n",
      "Epoch 149/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.4984 - val_loss: 98.3659\n",
      "Epoch 150/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.3991 - val_loss: 98.2090\n",
      "Epoch 151/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.2833 - val_loss: 98.0463\n",
      "Epoch 152/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.1698 - val_loss: 97.8716\n",
      "Epoch 153/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.0862 - val_loss: 97.7049\n",
      "Epoch 154/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.9784 - val_loss: 97.5485\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 2ms/step - loss: 88.8725 - val_loss: 97.3803\n",
      "Epoch 156/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.7825 - val_loss: 97.2379\n",
      "Epoch 157/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.6646 - val_loss: 97.0998\n",
      "Epoch 158/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.5882 - val_loss: 96.9791\n",
      "Epoch 159/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.4899 - val_loss: 96.8719\n",
      "Epoch 160/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.3948 - val_loss: 96.7286\n",
      "Epoch 161/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.3072 - val_loss: 96.5642\n",
      "Epoch 162/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.2211 - val_loss: 96.4123\n",
      "Epoch 163/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.1436 - val_loss: 96.2922\n",
      "Epoch 164/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.0569 - val_loss: 96.1588\n",
      "Epoch 165/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9695 - val_loss: 96.0345\n",
      "Epoch 166/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9054 - val_loss: 95.9102\n",
      "Epoch 167/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.8285 - val_loss: 95.7199\n",
      "Epoch 168/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.7357 - val_loss: 95.5478\n",
      "Epoch 169/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.6406 - val_loss: 95.3864\n",
      "Epoch 170/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.5612 - val_loss: 95.2510\n",
      "Epoch 171/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.4798 - val_loss: 95.1149\n",
      "Epoch 172/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.3995 - val_loss: 94.9432\n",
      "Epoch 173/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.3061 - val_loss: 94.7852\n",
      "Epoch 174/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.2281 - val_loss: 94.6796\n",
      "Epoch 175/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.1544 - val_loss: 94.5144\n",
      "Epoch 176/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.0623 - val_loss: 94.3721\n",
      "Epoch 177/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.9716 - val_loss: 94.2181\n",
      "Epoch 178/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.8825 - val_loss: 94.0622\n",
      "Epoch 179/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.7981 - val_loss: 93.9015\n",
      "Epoch 180/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.7103 - val_loss: 93.7492\n",
      "Epoch 181/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.6074 - val_loss: 93.6529\n",
      "Epoch 182/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.5323 - val_loss: 93.4842\n",
      "Epoch 183/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.4457 - val_loss: 93.3534\n",
      "Epoch 184/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.3421 - val_loss: 93.2163\n",
      "Epoch 185/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.2853 - val_loss: 93.0805\n",
      "Epoch 186/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.1984 - val_loss: 92.9638\n",
      "Epoch 187/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.0970 - val_loss: 92.7934\n",
      "Epoch 188/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.0042 - val_loss: 92.6543\n",
      "Epoch 189/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8818 - val_loss: 92.5501\n",
      "Epoch 190/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8092 - val_loss: 92.3999\n",
      "Epoch 191/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.7154 - val_loss: 92.3060\n",
      "Epoch 192/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.6332 - val_loss: 92.1905\n",
      "Epoch 193/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.5275 - val_loss: 92.0707\n",
      "Epoch 194/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.4525 - val_loss: 91.9159\n",
      "Epoch 195/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.3431 - val_loss: 91.7891\n",
      "Epoch 196/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.2676 - val_loss: 91.6578\n",
      "Epoch 197/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.1449 - val_loss: 91.5401\n",
      "Epoch 198/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.0660 - val_loss: 91.4460\n",
      "Epoch 199/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 84.9748 - val_loss: 91.3269\n",
      "Epoch 200/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 84.8665 - val_loss: 91.2029\n",
      "Epoch 201/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.7757 - val_loss: 91.0834\n",
      "Epoch 202/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.6713 - val_loss: 90.9566\n",
      "Epoch 203/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.5647 - val_loss: 90.7936\n",
      "Epoch 204/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.4629 - val_loss: 90.7135\n",
      "Epoch 205/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.3727 - val_loss: 90.5819\n",
      "Epoch 206/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2880 - val_loss: 90.4563\n",
      "Epoch 207/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2132 - val_loss: 90.3776\n",
      "Epoch 208/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.1274 - val_loss: 90.2397\n",
      "Epoch 209/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.0456 - val_loss: 90.1203\n",
      "Epoch 210/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.9580 - val_loss: 89.9896\n",
      "Epoch 211/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.8429 - val_loss: 89.8779\n",
      "Epoch 212/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.7532 - val_loss: 89.7788\n",
      "Epoch 213/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6989 - val_loss: 89.6668\n",
      "Epoch 214/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6161 - val_loss: 89.5365\n",
      "Epoch 215/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.5174 - val_loss: 89.4405\n",
      "Epoch 216/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4541 - val_loss: 89.3064\n",
      "Epoch 217/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.3713 - val_loss: 89.1623\n",
      "Epoch 218/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.2692 - val_loss: 89.0309\n",
      "Epoch 219/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.1942 - val_loss: 88.9174\n",
      "Epoch 220/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.1284 - val_loss: 88.8254\n",
      "Epoch 221/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.0656 - val_loss: 88.7246\n",
      "Epoch 222/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.9923 - val_loss: 88.6581\n",
      "Epoch 223/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.9453 - val_loss: 88.5753\n",
      "Epoch 224/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8852 - val_loss: 88.4395\n",
      "Epoch 225/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7778 - val_loss: 88.3159\n",
      "Epoch 226/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7036 - val_loss: 88.2085\n",
      "Epoch 227/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.6302 - val_loss: 88.0905\n",
      "Epoch 228/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.5779 - val_loss: 88.0249\n",
      "Epoch 229/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.5076 - val_loss: 87.9165\n",
      "Epoch 230/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.4426 - val_loss: 87.8139\n",
      "Epoch 231/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3874 - val_loss: 87.7078\n",
      "Epoch 232/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3043 - val_loss: 87.5873\n",
      "Epoch 233/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.2303 - val_loss: 87.4747\n",
      "Epoch 234/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.1823 - val_loss: 87.4040\n",
      "Epoch 235/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.1266 - val_loss: 87.3284\n",
      "Epoch 236/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0525 - val_loss: 87.2482\n",
      "Epoch 237/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0145 - val_loss: 87.1673\n",
      "Epoch 238/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.9691 - val_loss: 87.0637\n",
      "Epoch 239/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.8944 - val_loss: 86.9355\n",
      "Epoch 240/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.8294 - val_loss: 86.8626\n",
      "Epoch 241/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 81.7563 - val_loss: 86.7906\n",
      "Epoch 242/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 81.6935 - val_loss: 86.7109\n",
      "Epoch 243/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6564 - val_loss: 86.6278\n",
      "Epoch 244/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.5944 - val_loss: 86.5036\n",
      "Epoch 245/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.5302 - val_loss: 86.3972\n",
      "Epoch 246/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.4811 - val_loss: 86.3124\n",
      "Epoch 247/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.4163 - val_loss: 86.2168\n",
      "Epoch 248/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.3451 - val_loss: 86.0713\n",
      "Epoch 249/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.2846 - val_loss: 85.9915\n",
      "Epoch 250/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.2458 - val_loss: 85.8810\n",
      "Epoch 251/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.1671 - val_loss: 85.7584\n",
      "Epoch 252/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.1220 - val_loss: 85.7029\n",
      "Epoch 253/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0771 - val_loss: 85.6045\n",
      "Epoch 254/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0100 - val_loss: 85.5078\n",
      "Epoch 255/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.9329 - val_loss: 85.4434\n",
      "Epoch 256/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.9135 - val_loss: 85.3609\n",
      "Epoch 257/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.8169 - val_loss: 85.2538\n",
      "Epoch 258/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.7454 - val_loss: 85.1161\n",
      "Epoch 259/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.6622 - val_loss: 85.0009\n",
      "Epoch 260/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.5726 - val_loss: 84.8648\n",
      "Epoch 261/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.4959 - val_loss: 84.7679\n",
      "Epoch 262/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.4151 - val_loss: 84.6602\n",
      "Epoch 263/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.3258 - val_loss: 84.5609\n",
      "Epoch 264/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.2216 - val_loss: 84.4240\n",
      "Epoch 265/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.1506 - val_loss: 84.2894\n",
      "Epoch 266/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.0343 - val_loss: 84.1155\n",
      "Epoch 267/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.9494 - val_loss: 83.9824\n",
      "Epoch 268/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.8478 - val_loss: 83.8245\n",
      "Epoch 269/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.7091 - val_loss: 83.6474\n",
      "Epoch 270/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.6265 - val_loss: 83.5133\n",
      "Epoch 271/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.4893 - val_loss: 83.3286\n",
      "Epoch 272/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3788 - val_loss: 83.2016\n",
      "Epoch 273/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.2764 - val_loss: 83.0341\n",
      "Epoch 274/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.1535 - val_loss: 82.8483\n",
      "Epoch 275/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.0398 - val_loss: 82.6957\n",
      "Epoch 276/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.9109 - val_loss: 82.5574\n",
      "Epoch 277/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.7679 - val_loss: 82.3333\n",
      "Epoch 278/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.6533 - val_loss: 82.1580\n",
      "Epoch 279/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.5054 - val_loss: 82.0057\n",
      "Epoch 280/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.3829 - val_loss: 81.8163\n",
      "Epoch 281/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.2302 - val_loss: 81.5721\n",
      "Epoch 282/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.0340 - val_loss: 81.3913\n",
      "Epoch 283/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.9047 - val_loss: 81.2177\n",
      "Epoch 284/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 77.7796 - val_loss: 81.0499\n",
      "Epoch 285/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6259 - val_loss: 80.8267\n",
      "Epoch 286/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4426 - val_loss: 80.6303\n",
      "Epoch 287/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.2616 - val_loss: 80.3598\n",
      "Epoch 288/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0543 - val_loss: 80.0859\n",
      "Epoch 289/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8462 - val_loss: 79.7791\n",
      "Epoch 290/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.6227 - val_loss: 79.4794\n",
      "Epoch 291/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.3862 - val_loss: 79.1978\n",
      "Epoch 292/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.1250 - val_loss: 78.9007\n",
      "Epoch 293/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8745 - val_loss: 78.5751\n",
      "Epoch 294/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6179 - val_loss: 78.2656\n",
      "Epoch 295/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3494 - val_loss: 77.9391\n",
      "Epoch 296/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0805 - val_loss: 77.5571\n",
      "Epoch 297/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.7714 - val_loss: 77.2439\n",
      "Epoch 298/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.4720 - val_loss: 76.8774\n",
      "Epoch 299/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.1148 - val_loss: 76.4815\n",
      "Epoch 300/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7582 - val_loss: 76.0324\n",
      "Epoch 1/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 575.8497 - val_loss: 526.7895\n",
      "Epoch 2/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 553.3027 - val_loss: 502.8500\n",
      "Epoch 3/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 528.9717 - val_loss: 478.9078\n",
      "Epoch 4/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 506.0016 - val_loss: 456.2756\n",
      "Epoch 5/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 484.1352 - val_loss: 434.7702\n",
      "Epoch 6/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 463.3387 - val_loss: 414.2970\n",
      "Epoch 7/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 443.6219 - val_loss: 395.1081\n",
      "Epoch 8/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 425.2622 - val_loss: 377.3150\n",
      "Epoch 9/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 408.8214 - val_loss: 360.5201\n",
      "Epoch 10/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 392.9120 - val_loss: 344.8217\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 378.1674 - val_loss: 330.4044\n",
      "Epoch 12/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 364.5882 - val_loss: 316.9145\n",
      "Epoch 13/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 351.8046 - val_loss: 304.3401\n",
      "Epoch 14/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 340.4767 - val_loss: 292.8968\n",
      "Epoch 15/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 329.6390 - val_loss: 282.1321\n",
      "Epoch 16/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 319.6651 - val_loss: 272.3455\n",
      "Epoch 17/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 310.4013 - val_loss: 263.0394\n",
      "Epoch 18/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 301.6869 - val_loss: 254.6361\n",
      "Epoch 19/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 293.8368 - val_loss: 246.8756\n",
      "Epoch 20/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 286.4142 - val_loss: 239.4529\n",
      "Epoch 21/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 279.5334 - val_loss: 232.8278\n",
      "Epoch 22/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 273.3289 - val_loss: 226.4727\n",
      "Epoch 23/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 267.2744 - val_loss: 220.4881\n",
      "Epoch 24/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 261.6390 - val_loss: 214.9687\n",
      "Epoch 25/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 256.2626 - val_loss: 209.6083\n",
      "Epoch 26/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 251.2834 - val_loss: 204.5303\n",
      "Epoch 27/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 246.4296 - val_loss: 199.5964\n",
      "Epoch 28/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 241.7053 - val_loss: 194.9861\n",
      "Epoch 29/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 237.2780 - val_loss: 190.5174\n",
      "Epoch 30/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 233.0583 - val_loss: 186.2424\n",
      "Epoch 31/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 228.9144 - val_loss: 182.2414\n",
      "Epoch 32/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 225.0502 - val_loss: 178.4727\n",
      "Epoch 33/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 221.3206 - val_loss: 174.8183\n",
      "Epoch 34/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 217.7801 - val_loss: 171.3250\n",
      "Epoch 35/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 214.4237 - val_loss: 168.0177\n",
      "Epoch 36/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 211.1736 - val_loss: 164.7065\n",
      "Epoch 37/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 207.9723 - val_loss: 161.5238\n",
      "Epoch 38/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 204.9108 - val_loss: 158.4774\n",
      "Epoch 39/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 202.1281 - val_loss: 155.7025\n",
      "Epoch 40/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 199.4596 - val_loss: 152.9683\n",
      "Epoch 41/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 196.7910 - val_loss: 150.2901\n",
      "Epoch 42/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 194.2622 - val_loss: 147.7184\n",
      "Epoch 43/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 191.9096 - val_loss: 145.2958\n",
      "Epoch 44/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 189.5952 - val_loss: 142.9729\n",
      "Epoch 45/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 187.3605 - val_loss: 140.6358\n",
      "Epoch 46/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 185.2078 - val_loss: 138.3982\n",
      "Epoch 47/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 183.0482 - val_loss: 136.1975\n",
      "Epoch 48/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 181.1060 - val_loss: 133.9999\n",
      "Epoch 49/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 179.0783 - val_loss: 131.8914\n",
      "Epoch 50/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 177.1205 - val_loss: 129.8169\n",
      "Epoch 51/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 175.3889 - val_loss: 127.8093\n",
      "Epoch 52/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 173.5421 - val_loss: 125.8812\n",
      "Epoch 53/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 171.8659 - val_loss: 124.0027\n",
      "Epoch 54/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 170.1589 - val_loss: 122.1470\n",
      "Epoch 55/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 168.4955 - val_loss: 120.4054\n",
      "Epoch 56/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 166.9271 - val_loss: 118.7135\n",
      "Epoch 57/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 165.3578 - val_loss: 116.9482\n",
      "Epoch 58/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 163.8449 - val_loss: 115.3412\n",
      "Epoch 59/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 162.3880 - val_loss: 113.6457\n",
      "Epoch 60/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 161.0521 - val_loss: 111.9770\n",
      "Epoch 61/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 159.6763 - val_loss: 110.4565\n",
      "Epoch 62/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 158.3861 - val_loss: 108.9935\n",
      "Epoch 63/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 157.1109 - val_loss: 107.5908\n",
      "Epoch 64/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 155.8967 - val_loss: 106.1874\n",
      "Epoch 65/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 154.6476 - val_loss: 104.8619\n",
      "Epoch 66/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 153.4269 - val_loss: 103.6167\n",
      "Epoch 67/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 152.3215 - val_loss: 102.3205\n",
      "Epoch 68/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 151.1399 - val_loss: 101.1080\n",
      "Epoch 69/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 149.9465 - val_loss: 99.9072\n",
      "Epoch 70/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 148.8314 - val_loss: 98.7552\n",
      "Epoch 71/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 147.6998 - val_loss: 97.6742\n",
      "Epoch 72/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 146.5619 - val_loss: 96.5486\n",
      "Epoch 73/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 145.3919 - val_loss: 95.3882\n",
      "Epoch 74/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 144.1670 - val_loss: 94.3504\n",
      "Epoch 75/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 142.9538 - val_loss: 93.3448\n",
      "Epoch 76/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 141.6664 - val_loss: 92.2761\n",
      "Epoch 77/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 140.2874 - val_loss: 91.3395\n",
      "Epoch 78/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 138.9471 - val_loss: 90.2728\n",
      "Epoch 79/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 137.6418 - val_loss: 89.3177\n",
      "Epoch 80/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 136.2292 - val_loss: 88.2456\n",
      "Epoch 81/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 134.9421 - val_loss: 87.3535\n",
      "Epoch 82/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 133.5960 - val_loss: 86.3624\n",
      "Epoch 83/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 132.2543 - val_loss: 85.4818\n",
      "Epoch 84/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 130.8706 - val_loss: 84.6171\n",
      "Epoch 85/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 129.6428 - val_loss: 83.7371\n",
      "Epoch 86/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 128.4110 - val_loss: 82.8425\n",
      "Epoch 87/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 127.2301 - val_loss: 82.0569\n",
      "Epoch 88/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 125.9982 - val_loss: 81.1988\n",
      "Epoch 89/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 124.8509 - val_loss: 80.4068\n",
      "Epoch 90/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 123.7262 - val_loss: 79.6137\n",
      "Epoch 91/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 122.6694 - val_loss: 78.8677\n",
      "Epoch 92/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 121.6176 - val_loss: 78.1321\n",
      "Epoch 93/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 120.6944 - val_loss: 77.5014\n",
      "Epoch 94/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 119.8152 - val_loss: 76.9099\n",
      "Epoch 95/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 118.8887 - val_loss: 76.2949\n",
      "Epoch 96/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 118.0861 - val_loss: 75.6709\n",
      "Epoch 97/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 117.2586 - val_loss: 75.1558\n",
      "Epoch 98/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 116.5349 - val_loss: 74.6487\n",
      "Epoch 99/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 115.7875 - val_loss: 74.1746\n",
      "Epoch 100/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 115.0889 - val_loss: 73.7048\n",
      "Epoch 101/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.4209 - val_loss: 73.2582\n",
      "Epoch 102/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.7343 - val_loss: 72.8214\n",
      "Epoch 103/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.1390 - val_loss: 72.4000\n",
      "Epoch 104/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 112.5293 - val_loss: 72.0033\n",
      "Epoch 105/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.9241 - val_loss: 71.5644\n",
      "Epoch 106/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.3526 - val_loss: 71.1527\n",
      "Epoch 107/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.7630 - val_loss: 70.7555\n",
      "Epoch 108/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.2001 - val_loss: 70.3165\n",
      "Epoch 109/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.6582 - val_loss: 69.9274\n",
      "Epoch 110/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.1115 - val_loss: 69.5400\n",
      "Epoch 111/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.6114 - val_loss: 69.1720\n",
      "Epoch 112/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.1142 - val_loss: 68.8404\n",
      "Epoch 113/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.5997 - val_loss: 68.5142\n",
      "Epoch 114/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.1329 - val_loss: 68.1782\n",
      "Epoch 115/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.6910 - val_loss: 67.8582\n",
      "Epoch 116/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.2402 - val_loss: 67.5190\n",
      "Epoch 117/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.7494 - val_loss: 67.1589\n",
      "Epoch 118/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.2927 - val_loss: 66.8086\n",
      "Epoch 119/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.8649 - val_loss: 66.4886\n",
      "Epoch 120/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.4840 - val_loss: 66.1451\n",
      "Epoch 121/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.0791 - val_loss: 65.8572\n",
      "Epoch 122/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.6575 - val_loss: 65.5403\n",
      "Epoch 123/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.2636 - val_loss: 65.2524\n",
      "Epoch 124/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.8817 - val_loss: 65.0012\n",
      "Epoch 125/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.5305 - val_loss: 64.7386\n",
      "Epoch 126/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.1900 - val_loss: 64.4530\n",
      "Epoch 127/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.8504 - val_loss: 64.1384\n",
      "Epoch 128/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.4714 - val_loss: 63.8952\n",
      "Epoch 129/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.1575 - val_loss: 63.6594\n",
      "Epoch 130/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.8099 - val_loss: 63.3517\n",
      "Epoch 131/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.4855 - val_loss: 63.1006\n",
      "Epoch 132/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.1824 - val_loss: 62.9043\n",
      "Epoch 133/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.8884 - val_loss: 62.6576\n",
      "Epoch 134/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.6009 - val_loss: 62.4519\n",
      "Epoch 135/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.2895 - val_loss: 62.2083\n",
      "Epoch 136/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.0306 - val_loss: 62.0412\n",
      "Epoch 137/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.7441 - val_loss: 61.8666\n",
      "Epoch 138/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.4550 - val_loss: 61.6545\n",
      "Epoch 139/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.1944 - val_loss: 61.4337\n",
      "Epoch 140/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.8813 - val_loss: 61.2291\n",
      "Epoch 141/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.6438 - val_loss: 61.0138\n",
      "Epoch 142/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.3713 - val_loss: 60.7953\n",
      "Epoch 143/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.1044 - val_loss: 60.6215\n",
      "Epoch 144/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.8684 - val_loss: 60.4349\n",
      "Epoch 145/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.6372 - val_loss: 60.2615\n",
      "Epoch 146/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.3701 - val_loss: 60.0733\n",
      "Epoch 147/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.1475 - val_loss: 59.8932\n",
      "Epoch 148/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.9129 - val_loss: 59.7399\n",
      "Epoch 149/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.6399 - val_loss: 59.5331\n",
      "Epoch 150/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.3825 - val_loss: 59.3988\n",
      "Epoch 151/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 95.1568 - val_loss: 59.2691\n",
      "Epoch 152/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 94.9327 - val_loss: 59.1250\n",
      "Epoch 153/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.7070 - val_loss: 58.9650\n",
      "Epoch 154/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.4783 - val_loss: 58.7986\n",
      "Epoch 155/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.2190 - val_loss: 58.6337\n",
      "Epoch 156/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.0201 - val_loss: 58.5018\n",
      "Epoch 157/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.7757 - val_loss: 58.3432\n",
      "Epoch 158/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.5356 - val_loss: 58.1951\n",
      "Epoch 159/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.3190 - val_loss: 58.0625\n",
      "Epoch 160/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.0922 - val_loss: 57.9137\n",
      "Epoch 161/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.8931 - val_loss: 57.7619\n",
      "Epoch 162/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.6564 - val_loss: 57.6156\n",
      "Epoch 163/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.4173 - val_loss: 57.4718\n",
      "Epoch 164/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.1740 - val_loss: 57.3530\n",
      "Epoch 165/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.9422 - val_loss: 57.2266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.7405 - val_loss: 57.0797\n",
      "Epoch 167/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.5373 - val_loss: 56.9025\n",
      "Epoch 168/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.3286 - val_loss: 56.7678\n",
      "Epoch 169/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.1068 - val_loss: 56.5816\n",
      "Epoch 170/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.9147 - val_loss: 56.4332\n",
      "Epoch 171/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.7410 - val_loss: 56.3136\n",
      "Epoch 172/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.5381 - val_loss: 56.1210\n",
      "Epoch 173/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.3161 - val_loss: 55.9967\n",
      "Epoch 174/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.1171 - val_loss: 55.8604\n",
      "Epoch 175/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.9189 - val_loss: 55.7057\n",
      "Epoch 176/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.7419 - val_loss: 55.5518\n",
      "Epoch 177/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.5326 - val_loss: 55.4297\n",
      "Epoch 178/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.3661 - val_loss: 55.2647\n",
      "Epoch 179/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.1746 - val_loss: 55.1385\n",
      "Epoch 180/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.9893 - val_loss: 54.9999\n",
      "Epoch 181/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.8613 - val_loss: 54.8558\n",
      "Epoch 182/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.6891 - val_loss: 54.6852\n",
      "Epoch 183/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.4964 - val_loss: 54.5347\n",
      "Epoch 184/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.3293 - val_loss: 54.3485\n",
      "Epoch 185/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.2009 - val_loss: 54.2107\n",
      "Epoch 186/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.0580 - val_loss: 54.0535\n",
      "Epoch 187/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9110 - val_loss: 53.8962\n",
      "Epoch 188/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.7473 - val_loss: 53.7383\n",
      "Epoch 189/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.5977 - val_loss: 53.6175\n",
      "Epoch 190/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.5177 - val_loss: 53.4842\n",
      "Epoch 191/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.3753 - val_loss: 53.3560\n",
      "Epoch 192/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.2321 - val_loss: 53.2391\n",
      "Epoch 193/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.0993 - val_loss: 53.1253\n",
      "Epoch 194/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.9743 - val_loss: 52.9806\n",
      "Epoch 195/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.8876 - val_loss: 52.9034\n",
      "Epoch 196/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 86.7622 - val_loss: 52.7705\n",
      "Epoch 197/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.6105 - val_loss: 52.6440\n",
      "Epoch 198/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.4949 - val_loss: 52.5544\n",
      "Epoch 199/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.3617 - val_loss: 52.4903\n",
      "Epoch 200/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.2955 - val_loss: 52.3373\n",
      "Epoch 201/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.1494 - val_loss: 52.2334\n",
      "Epoch 202/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.0306 - val_loss: 52.1557\n",
      "Epoch 203/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.9553 - val_loss: 52.0666\n",
      "Epoch 204/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8679 - val_loss: 51.9320\n",
      "Epoch 205/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8003 - val_loss: 51.8265\n",
      "Epoch 206/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.7103 - val_loss: 51.7119\n",
      "Epoch 207/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.6471 - val_loss: 51.6172\n",
      "Epoch 208/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.5290 - val_loss: 51.5382\n",
      "Epoch 209/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.4087 - val_loss: 51.4288\n",
      "Epoch 210/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.3191 - val_loss: 51.3575\n",
      "Epoch 211/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.2314 - val_loss: 51.2954\n",
      "Epoch 212/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.1253 - val_loss: 51.2484\n",
      "Epoch 213/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.0525 - val_loss: 51.1755\n",
      "Epoch 214/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.9894 - val_loss: 51.1030\n",
      "Epoch 215/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.9057 - val_loss: 51.0224\n",
      "Epoch 216/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.8141 - val_loss: 50.9630\n",
      "Epoch 217/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.7402 - val_loss: 50.8765\n",
      "Epoch 218/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.6426 - val_loss: 50.8239\n",
      "Epoch 219/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.5824 - val_loss: 50.7634\n",
      "Epoch 220/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.4595 - val_loss: 50.7041\n",
      "Epoch 221/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.3919 - val_loss: 50.6430\n",
      "Epoch 222/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2898 - val_loss: 50.5648\n",
      "Epoch 223/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2436 - val_loss: 50.5157\n",
      "Epoch 224/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2013 - val_loss: 50.4699\n",
      "Epoch 225/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.1250 - val_loss: 50.4151\n",
      "Epoch 226/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.0275 - val_loss: 50.3987\n",
      "Epoch 227/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.9417 - val_loss: 50.3517\n",
      "Epoch 228/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.8899 - val_loss: 50.2975\n",
      "Epoch 229/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.8112 - val_loss: 50.2744\n",
      "Epoch 230/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.7281 - val_loss: 50.2540\n",
      "Epoch 231/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.7001 - val_loss: 50.1687\n",
      "Epoch 232/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6064 - val_loss: 50.1398\n",
      "Epoch 233/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.5339 - val_loss: 50.0672\n",
      "Epoch 234/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4726 - val_loss: 50.0300\n",
      "Epoch 235/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4074 - val_loss: 49.9845\n",
      "Epoch 236/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.3383 - val_loss: 49.9399\n",
      "Epoch 237/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.2676 - val_loss: 49.8885\n",
      "Epoch 238/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.2030 - val_loss: 49.8449\n",
      "Epoch 239/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 83.1207 - val_loss: 49.8065\n",
      "Epoch 240/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 83.0547 - val_loss: 49.7921\n",
      "Epoch 241/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 82.9586 - val_loss: 49.7603\n",
      "Epoch 242/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8738 - val_loss: 49.7215\n",
      "Epoch 243/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8206 - val_loss: 49.6928\n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7525 - val_loss: 49.6413\n",
      "Epoch 245/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7014 - val_loss: 49.5866\n",
      "Epoch 246/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.6279 - val_loss: 49.5633\n",
      "Epoch 247/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.5673 - val_loss: 49.4842\n",
      "Epoch 248/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.4915 - val_loss: 49.4177\n",
      "Epoch 249/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.4331 - val_loss: 49.3623\n",
      "Epoch 250/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3404 - val_loss: 49.3297\n",
      "Epoch 251/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3019 - val_loss: 49.2888\n",
      "Epoch 252/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.2583 - val_loss: 49.2240\n",
      "Epoch 253/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.1417 - val_loss: 49.1931\n",
      "Epoch 254/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0505 - val_loss: 49.1669\n",
      "Epoch 255/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.9887 - val_loss: 49.1250\n",
      "Epoch 256/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.8876 - val_loss: 49.0979\n",
      "Epoch 257/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.7989 - val_loss: 49.0756\n",
      "Epoch 258/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6701 - val_loss: 49.0363\n",
      "Epoch 259/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6011 - val_loss: 49.0245\n",
      "Epoch 260/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.5010 - val_loss: 49.0044\n",
      "Epoch 261/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.3723 - val_loss: 48.9952\n",
      "Epoch 262/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.2810 - val_loss: 49.0036\n",
      "Epoch 263/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.1917 - val_loss: 49.0028\n",
      "Epoch 264/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0752 - val_loss: 48.9666\n",
      "Epoch 265/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.9946 - val_loss: 48.9679\n",
      "Epoch 266/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.8556 - val_loss: 48.9587\n",
      "Epoch 267/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.7429 - val_loss: 48.9436\n",
      "Epoch 268/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.6308 - val_loss: 48.8975\n",
      "Epoch 269/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.5047 - val_loss: 48.8356\n",
      "Epoch 270/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.3476 - val_loss: 48.8363\n",
      "Epoch 271/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.1923 - val_loss: 48.8032\n",
      "Epoch 272/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.0495 - val_loss: 48.7881\n",
      "Epoch 273/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.8797 - val_loss: 48.7510\n",
      "Epoch 274/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.7066 - val_loss: 48.6758\n",
      "Epoch 275/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.5510 - val_loss: 48.6353\n",
      "Epoch 276/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3564 - val_loss: 48.5880\n",
      "Epoch 277/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.1683 - val_loss: 48.5534\n",
      "Epoch 278/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.9625 - val_loss: 48.5025\n",
      "Epoch 279/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.7432 - val_loss: 48.4589\n",
      "Epoch 280/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.4827 - val_loss: 48.4105\n",
      "Epoch 281/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.2047 - val_loss: 48.3468\n",
      "Epoch 282/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.9262 - val_loss: 48.2625\n",
      "Epoch 283/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.5750 - val_loss: 48.2177\n",
      "Epoch 284/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 77.1876 - val_loss: 48.1148\n",
      "Epoch 285/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 76.7894 - val_loss: 48.0348\n",
      "Epoch 286/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.3341 - val_loss: 47.9684\n",
      "Epoch 287/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8431 - val_loss: 47.8329\n",
      "Epoch 288/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4244 - val_loss: 47.6768\n",
      "Epoch 289/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8523 - val_loss: 47.5408\n",
      "Epoch 290/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.2810 - val_loss: 47.3600\n",
      "Epoch 291/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7182 - val_loss: 47.2250\n",
      "Epoch 292/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.1039 - val_loss: 47.0204\n",
      "Epoch 293/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.4643 - val_loss: 46.8271\n",
      "Epoch 294/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.8475 - val_loss: 46.6330\n",
      "Epoch 295/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.1099 - val_loss: 46.4640\n",
      "Epoch 296/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.3793 - val_loss: 46.2588\n",
      "Epoch 297/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.6574 - val_loss: 46.0744\n",
      "Epoch 298/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 68.8091 - val_loss: 45.9043\n",
      "Epoch 299/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 68.0688 - val_loss: 45.7257\n",
      "Epoch 300/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 67.2197 - val_loss: 45.5376\n",
      "Epoch 1/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 586.3391 - val_loss: 495.6230\n",
      "Epoch 2/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 562.8732 - val_loss: 474.8716\n",
      "Epoch 3/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 537.6857 - val_loss: 454.3872\n",
      "Epoch 4/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 513.4221 - val_loss: 434.8630\n",
      "Epoch 5/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 490.3528 - val_loss: 416.4720\n",
      "Epoch 6/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 468.7455 - val_loss: 399.1580\n",
      "Epoch 7/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 447.7480 - val_loss: 382.9336\n",
      "Epoch 8/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 428.8385 - val_loss: 367.8272\n",
      "Epoch 9/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 410.6445 - val_loss: 353.5605\n",
      "Epoch 10/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 393.8206 - val_loss: 340.4072\n",
      "Epoch 11/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 378.3408 - val_loss: 327.9887\n",
      "Epoch 12/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 363.7504 - val_loss: 316.6165\n",
      "Epoch 13/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 349.9694 - val_loss: 305.6531\n",
      "Epoch 14/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 337.4784 - val_loss: 295.5394\n",
      "Epoch 15/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 325.7439 - val_loss: 286.3543\n",
      "Epoch 16/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 314.7878 - val_loss: 277.6985\n",
      "Epoch 17/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 305.1566 - val_loss: 269.7931\n",
      "Epoch 18/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 295.6624 - val_loss: 262.3335\n",
      "Epoch 19/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 287.4100 - val_loss: 255.3802\n",
      "Epoch 20/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 279.3566 - val_loss: 248.9874\n",
      "Epoch 21/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 272.0524 - val_loss: 242.9147\n",
      "Epoch 22/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 265.1563 - val_loss: 237.2316\n",
      "Epoch 23/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 258.5571 - val_loss: 231.7927\n",
      "Epoch 24/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 252.2722 - val_loss: 226.5998\n",
      "Epoch 25/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 246.1755 - val_loss: 221.6696\n",
      "Epoch 26/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 240.4185 - val_loss: 216.9635\n",
      "Epoch 27/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 235.0818 - val_loss: 212.5063\n",
      "Epoch 28/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 230.0471 - val_loss: 208.2501\n",
      "Epoch 29/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 224.9032 - val_loss: 204.2135\n",
      "Epoch 30/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 220.2923 - val_loss: 200.3871\n",
      "Epoch 31/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 215.7683 - val_loss: 196.6911\n",
      "Epoch 32/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 211.4236 - val_loss: 193.1267\n",
      "Epoch 33/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 207.0842 - val_loss: 189.7886\n",
      "Epoch 34/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 203.1971 - val_loss: 186.5780\n",
      "Epoch 35/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 199.3912 - val_loss: 183.6390\n",
      "Epoch 36/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 195.8474 - val_loss: 180.7730\n",
      "Epoch 37/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 192.3224 - val_loss: 178.0939\n",
      "Epoch 38/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 188.9435 - val_loss: 175.4607\n",
      "Epoch 39/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 185.7521 - val_loss: 172.9825\n",
      "Epoch 40/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 182.5948 - val_loss: 170.5768\n",
      "Epoch 41/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 179.5665 - val_loss: 168.2744\n",
      "Epoch 42/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 176.6311 - val_loss: 166.0697\n",
      "Epoch 43/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 173.8165 - val_loss: 163.9466\n",
      "Epoch 44/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 171.0902 - val_loss: 161.9558\n",
      "Epoch 45/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 168.4883 - val_loss: 160.0270\n",
      "Epoch 46/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 165.8719 - val_loss: 158.2040\n",
      "Epoch 47/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 163.3762 - val_loss: 156.4290\n",
      "Epoch 48/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 160.9513 - val_loss: 154.7694\n",
      "Epoch 49/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 158.6276 - val_loss: 153.1447\n",
      "Epoch 50/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 156.4084 - val_loss: 151.5785\n",
      "Epoch 51/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 154.1283 - val_loss: 150.0614\n",
      "Epoch 52/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 152.0337 - val_loss: 148.5913\n",
      "Epoch 53/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 149.9915 - val_loss: 147.2014\n",
      "Epoch 54/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 148.0666 - val_loss: 145.8129\n",
      "Epoch 55/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 146.0266 - val_loss: 144.4715\n",
      "Epoch 56/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 144.2016 - val_loss: 143.1663\n",
      "Epoch 57/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 142.3617 - val_loss: 141.8623\n",
      "Epoch 58/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 140.5881 - val_loss: 140.5910\n",
      "Epoch 59/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 138.8927 - val_loss: 139.3240\n",
      "Epoch 60/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 137.1461 - val_loss: 138.0406\n",
      "Epoch 61/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 135.4348 - val_loss: 136.7165\n",
      "Epoch 62/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 133.8440 - val_loss: 135.4680\n",
      "Epoch 63/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 132.2702 - val_loss: 134.2643\n",
      "Epoch 64/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 130.7698 - val_loss: 133.0695\n",
      "Epoch 65/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 129.2264 - val_loss: 131.9135\n",
      "Epoch 66/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 127.7155 - val_loss: 130.8092\n",
      "Epoch 67/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 126.2761 - val_loss: 129.7367\n",
      "Epoch 68/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 124.9211 - val_loss: 128.7379\n",
      "Epoch 69/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 123.5688 - val_loss: 127.7757\n",
      "Epoch 70/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 122.2998 - val_loss: 126.8474\n",
      "Epoch 71/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 121.0888 - val_loss: 125.9594\n",
      "Epoch 72/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 119.9343 - val_loss: 125.0944\n",
      "Epoch 73/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 118.8740 - val_loss: 124.2957\n",
      "Epoch 74/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 117.8601 - val_loss: 123.4993\n",
      "Epoch 75/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 116.8451 - val_loss: 122.7164\n",
      "Epoch 76/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 115.8784 - val_loss: 121.9779\n",
      "Epoch 77/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.9897 - val_loss: 121.2421\n",
      "Epoch 78/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.0674 - val_loss: 120.5064\n",
      "Epoch 79/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.2672 - val_loss: 119.8240\n",
      "Epoch 80/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 112.4177 - val_loss: 119.1824\n",
      "Epoch 81/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.6368 - val_loss: 118.5437\n",
      "Epoch 82/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.8334 - val_loss: 117.8965\n",
      "Epoch 83/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.0999 - val_loss: 117.2619\n",
      "Epoch 84/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.3381 - val_loss: 116.6413\n",
      "Epoch 85/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.5894 - val_loss: 116.0412\n",
      "Epoch 86/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.8644 - val_loss: 115.4301\n",
      "Epoch 87/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 107.1919 - val_loss: 114.8352\n",
      "Epoch 88/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.5440 - val_loss: 114.2418\n",
      "Epoch 89/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.8764 - val_loss: 113.6709\n",
      "Epoch 90/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.2393 - val_loss: 113.1217\n",
      "Epoch 91/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.5987 - val_loss: 112.5483\n",
      "Epoch 92/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.0146 - val_loss: 112.0360\n",
      "Epoch 93/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.4051 - val_loss: 111.4789\n",
      "Epoch 94/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.8168 - val_loss: 110.9608\n",
      "Epoch 95/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.2137 - val_loss: 110.4628\n",
      "Epoch 96/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.6594 - val_loss: 109.9701\n",
      "Epoch 97/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.1093 - val_loss: 109.5112\n",
      "Epoch 98/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.5983 - val_loss: 109.0628\n",
      "Epoch 99/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.9914 - val_loss: 108.5839\n",
      "Epoch 100/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.4896 - val_loss: 108.1198\n",
      "Epoch 101/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.0055 - val_loss: 107.6901\n",
      "Epoch 102/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.4510 - val_loss: 107.2544\n",
      "Epoch 103/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.9619 - val_loss: 106.8414\n",
      "Epoch 104/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.5229 - val_loss: 106.4563\n",
      "Epoch 105/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.0553 - val_loss: 106.1031\n",
      "Epoch 106/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.5898 - val_loss: 105.7359\n",
      "Epoch 107/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.1636 - val_loss: 105.4180\n",
      "Epoch 108/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.7661 - val_loss: 105.0731\n",
      "Epoch 109/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.3128 - val_loss: 104.7368\n",
      "Epoch 110/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.9013 - val_loss: 104.4105\n",
      "Epoch 111/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.4995 - val_loss: 104.0812\n",
      "Epoch 112/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.1126 - val_loss: 103.7942\n",
      "Epoch 113/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 93.7303 - val_loss: 103.4586\n",
      "Epoch 114/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 93.3762 - val_loss: 103.1602\n",
      "Epoch 115/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.0232 - val_loss: 102.8869\n",
      "Epoch 116/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.6870 - val_loss: 102.6086\n",
      "Epoch 117/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.3459 - val_loss: 102.3418\n",
      "Epoch 118/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.0098 - val_loss: 102.0961\n",
      "Epoch 119/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.6686 - val_loss: 101.8507\n",
      "Epoch 120/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.3355 - val_loss: 101.6243\n",
      "Epoch 121/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.0426 - val_loss: 101.4123\n",
      "Epoch 122/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.7528 - val_loss: 101.1820\n",
      "Epoch 123/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.4505 - val_loss: 100.9654\n",
      "Epoch 124/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.1634 - val_loss: 100.7473\n",
      "Epoch 125/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.9121 - val_loss: 100.5213\n",
      "Epoch 126/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.6435 - val_loss: 100.3121\n",
      "Epoch 127/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.3569 - val_loss: 100.1156\n",
      "Epoch 128/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.1201 - val_loss: 99.9057\n",
      "Epoch 129/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.8554 - val_loss: 99.6913\n",
      "Epoch 130/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.6201 - val_loss: 99.5109\n",
      "Epoch 131/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.3836 - val_loss: 99.3036\n",
      "Epoch 132/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.1585 - val_loss: 99.1309\n",
      "Epoch 133/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9487 - val_loss: 98.9628\n",
      "Epoch 134/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.7186 - val_loss: 98.7632\n",
      "Epoch 135/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.5300 - val_loss: 98.5892\n",
      "Epoch 136/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.3378 - val_loss: 98.4199\n",
      "Epoch 137/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.1458 - val_loss: 98.2445\n",
      "Epoch 138/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.9596 - val_loss: 98.0792\n",
      "Epoch 139/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.7528 - val_loss: 97.9135\n",
      "Epoch 140/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.5611 - val_loss: 97.7438\n",
      "Epoch 141/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.3806 - val_loss: 97.5591\n",
      "Epoch 142/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.1988 - val_loss: 97.4035\n",
      "Epoch 143/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.0235 - val_loss: 97.2207\n",
      "Epoch 144/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.8416 - val_loss: 97.0275\n",
      "Epoch 145/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.6699 - val_loss: 96.8660\n",
      "Epoch 146/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.5004 - val_loss: 96.7017\n",
      "Epoch 147/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.3359 - val_loss: 96.5398\n",
      "Epoch 148/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.1575 - val_loss: 96.3707\n",
      "Epoch 149/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.9770 - val_loss: 96.2107\n",
      "Epoch 150/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.8129 - val_loss: 96.0629\n",
      "Epoch 151/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.6569 - val_loss: 95.9306\n",
      "Epoch 152/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.5182 - val_loss: 95.8001\n",
      "Epoch 153/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.3744 - val_loss: 95.6363\n",
      "Epoch 154/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2117 - val_loss: 95.5087\n",
      "Epoch 155/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.0696 - val_loss: 95.3572\n",
      "Epoch 156/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 83.9272 - val_loss: 95.2404\n",
      "Epoch 157/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 83.7895 - val_loss: 95.1195\n",
      "Epoch 158/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6323 - val_loss: 95.0038\n",
      "Epoch 159/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.4736 - val_loss: 94.8894\n",
      "Epoch 160/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.3557 - val_loss: 94.7778\n",
      "Epoch 161/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.2157 - val_loss: 94.6736\n",
      "Epoch 162/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.0902 - val_loss: 94.5530\n",
      "Epoch 163/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.9746 - val_loss: 94.4556\n",
      "Epoch 164/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8396 - val_loss: 94.3459\n",
      "Epoch 165/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.7038 - val_loss: 94.1882\n",
      "Epoch 166/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.5811 - val_loss: 94.0755\n",
      "Epoch 167/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.4513 - val_loss: 93.9865\n",
      "Epoch 168/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3277 - val_loss: 93.8578\n",
      "Epoch 169/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.2043 - val_loss: 93.7215\n",
      "Epoch 170/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0649 - val_loss: 93.6082\n",
      "Epoch 171/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.9604 - val_loss: 93.4821\n",
      "Epoch 172/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.8106 - val_loss: 93.3876\n",
      "Epoch 173/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6731 - val_loss: 93.3043\n",
      "Epoch 174/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.5287 - val_loss: 93.2016\n",
      "Epoch 175/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.3809 - val_loss: 93.0986\n",
      "Epoch 176/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.2644 - val_loss: 93.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.1631 - val_loss: 92.9349\n",
      "Epoch 178/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0061 - val_loss: 92.8421\n",
      "Epoch 179/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.8792 - val_loss: 92.7126\n",
      "Epoch 180/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.7301 - val_loss: 92.6187\n",
      "Epoch 181/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.5794 - val_loss: 92.4949\n",
      "Epoch 182/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.4497 - val_loss: 92.3437\n",
      "Epoch 183/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.3072 - val_loss: 92.2264\n",
      "Epoch 184/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.1876 - val_loss: 92.0919\n",
      "Epoch 185/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.0336 - val_loss: 91.9469\n",
      "Epoch 186/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.8882 - val_loss: 91.8253\n",
      "Epoch 187/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.7669 - val_loss: 91.7122\n",
      "Epoch 188/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.6125 - val_loss: 91.5950\n",
      "Epoch 189/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.4830 - val_loss: 91.4347\n",
      "Epoch 190/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3585 - val_loss: 91.3250\n",
      "Epoch 191/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.2285 - val_loss: 91.2042\n",
      "Epoch 192/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.0819 - val_loss: 91.0498\n",
      "Epoch 193/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.9553 - val_loss: 90.9121\n",
      "Epoch 194/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.8114 - val_loss: 90.8026\n",
      "Epoch 195/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.6621 - val_loss: 90.6996\n",
      "Epoch 196/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.5217 - val_loss: 90.5921\n",
      "Epoch 197/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.4027 - val_loss: 90.4862\n",
      "Epoch 198/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.2595 - val_loss: 90.3526\n",
      "Epoch 199/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.1675 - val_loss: 90.2363\n",
      "Epoch 200/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 78.0398 - val_loss: 90.1137\n",
      "Epoch 201/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 77.9308 - val_loss: 89.9746\n",
      "Epoch 202/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.7995 - val_loss: 89.8515\n",
      "Epoch 203/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6707 - val_loss: 89.7267\n",
      "Epoch 204/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.5632 - val_loss: 89.5826\n",
      "Epoch 205/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4164 - val_loss: 89.4082\n",
      "Epoch 206/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.2949 - val_loss: 89.2565\n",
      "Epoch 207/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.1324 - val_loss: 89.0901\n",
      "Epoch 208/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.9973 - val_loss: 88.9542\n",
      "Epoch 209/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8518 - val_loss: 88.8130\n",
      "Epoch 210/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.7120 - val_loss: 88.7135\n",
      "Epoch 211/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.5384 - val_loss: 88.5837\n",
      "Epoch 212/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4025 - val_loss: 88.4953\n",
      "Epoch 213/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.2516 - val_loss: 88.3647\n",
      "Epoch 214/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0964 - val_loss: 88.2567\n",
      "Epoch 215/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.9442 - val_loss: 88.1580\n",
      "Epoch 216/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8191 - val_loss: 88.0296\n",
      "Epoch 217/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6799 - val_loss: 87.9337\n",
      "Epoch 218/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.5175 - val_loss: 87.8087\n",
      "Epoch 219/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3828 - val_loss: 87.7007\n",
      "Epoch 220/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.2632 - val_loss: 87.6202\n",
      "Epoch 221/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1081 - val_loss: 87.5074\n",
      "Epoch 222/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.9741 - val_loss: 87.4424\n",
      "Epoch 223/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8571 - val_loss: 87.3408\n",
      "Epoch 224/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.7503 - val_loss: 87.2338\n",
      "Epoch 225/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.6329 - val_loss: 87.0954\n",
      "Epoch 226/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.5183 - val_loss: 86.9902\n",
      "Epoch 227/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.3881 - val_loss: 86.8722\n",
      "Epoch 228/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.2847 - val_loss: 86.7855\n",
      "Epoch 229/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.1606 - val_loss: 86.7023\n",
      "Epoch 230/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.0358 - val_loss: 86.6292\n",
      "Epoch 231/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.9472 - val_loss: 86.5084\n",
      "Epoch 232/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.8370 - val_loss: 86.4044\n",
      "Epoch 233/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7246 - val_loss: 86.2975\n",
      "Epoch 234/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.6189 - val_loss: 86.1967\n",
      "Epoch 235/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.5221 - val_loss: 86.1070\n",
      "Epoch 236/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.4262 - val_loss: 86.0399\n",
      "Epoch 237/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.3444 - val_loss: 85.9110\n",
      "Epoch 238/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.2304 - val_loss: 85.8165\n",
      "Epoch 239/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.1277 - val_loss: 85.6986\n",
      "Epoch 240/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.0401 - val_loss: 85.5823\n",
      "Epoch 241/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.8929 - val_loss: 85.4828\n",
      "Epoch 242/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.7986 - val_loss: 85.3628\n",
      "Epoch 243/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 72.6957 - val_loss: 85.2656\n",
      "Epoch 244/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 72.6276 - val_loss: 85.1470\n",
      "Epoch 245/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.5177 - val_loss: 85.0592\n",
      "Epoch 246/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.4429 - val_loss: 84.9211\n",
      "Epoch 247/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.3365 - val_loss: 84.8132\n",
      "Epoch 248/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.2357 - val_loss: 84.6854\n",
      "Epoch 249/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.1471 - val_loss: 84.5730\n",
      "Epoch 250/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 72.0713 - val_loss: 84.4681\n",
      "Epoch 251/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.9645 - val_loss: 84.3502\n",
      "Epoch 252/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.8852 - val_loss: 84.2684\n",
      "Epoch 253/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.7790 - val_loss: 84.1499\n",
      "Epoch 254/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.6651 - val_loss: 84.0979\n",
      "Epoch 255/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 71.5755 - val_loss: 83.9653\n",
      "Epoch 256/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.5389 - val_loss: 83.8448\n",
      "Epoch 257/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.4317 - val_loss: 83.7365\n",
      "Epoch 258/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.3546 - val_loss: 83.5979\n",
      "Epoch 259/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.2753 - val_loss: 83.4143\n",
      "Epoch 260/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.1419 - val_loss: 83.2962\n",
      "Epoch 261/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 71.0604 - val_loss: 83.1845\n",
      "Epoch 262/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.9451 - val_loss: 83.0341\n",
      "Epoch 263/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.8119 - val_loss: 82.8454\n",
      "Epoch 264/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.7030 - val_loss: 82.7310\n",
      "Epoch 265/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.6088 - val_loss: 82.5715\n",
      "Epoch 266/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.4743 - val_loss: 82.3708\n",
      "Epoch 267/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.3497 - val_loss: 82.1988\n",
      "Epoch 268/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.1858 - val_loss: 82.0154\n",
      "Epoch 269/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 70.0265 - val_loss: 81.8198\n",
      "Epoch 270/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.8900 - val_loss: 81.6140\n",
      "Epoch 271/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.7181 - val_loss: 81.4640\n",
      "Epoch 272/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.5303 - val_loss: 81.2316\n",
      "Epoch 273/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.3479 - val_loss: 81.0021\n",
      "Epoch 274/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 69.1306 - val_loss: 80.7576\n",
      "Epoch 275/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 68.8755 - val_loss: 80.5292\n",
      "Epoch 276/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 68.6055 - val_loss: 80.2272\n",
      "Epoch 277/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 68.2785 - val_loss: 79.8713\n",
      "Epoch 278/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 67.9657 - val_loss: 79.5559\n",
      "Epoch 279/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 67.6193 - val_loss: 79.2158\n",
      "Epoch 280/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 67.2640 - val_loss: 78.8455\n",
      "Epoch 281/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 66.8959 - val_loss: 78.4727\n",
      "Epoch 282/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 66.5120 - val_loss: 78.0719\n",
      "Epoch 283/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 66.1660 - val_loss: 77.7195\n",
      "Epoch 284/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 65.7592 - val_loss: 77.2602\n",
      "Epoch 285/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 65.3106 - val_loss: 76.8028\n",
      "Epoch 286/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 64.8414 - val_loss: 76.2586\n",
      "Epoch 287/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 64.3626 - val_loss: 75.7189\n",
      "Epoch 288/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 63.8606 - val_loss: 75.1620\n",
      "Epoch 289/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 63.3438 - val_loss: 74.5683\n",
      "Epoch 290/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 62.8367 - val_loss: 73.9841\n",
      "Epoch 291/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 62.2611 - val_loss: 73.3478\n",
      "Epoch 292/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 61.7319 - val_loss: 72.7529\n",
      "Epoch 293/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 61.1628 - val_loss: 72.2169\n",
      "Epoch 294/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 60.6175 - val_loss: 71.6402\n",
      "Epoch 295/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 60.0492 - val_loss: 71.0547\n",
      "Epoch 296/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 59.4769 - val_loss: 70.4070\n",
      "Epoch 297/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 58.8916 - val_loss: 69.7595\n",
      "Epoch 298/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 58.2993 - val_loss: 69.1522\n",
      "Epoch 299/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 57.6807 - val_loss: 68.5344\n",
      "Epoch 300/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 57.1189 - val_loss: 67.9045\n",
      "Epoch 1/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 532.8508 - val_loss: 656.5676\n",
      "Epoch 2/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 509.1707 - val_loss: 628.7330\n",
      "Epoch 3/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 484.6044 - val_loss: 601.3826\n",
      "Epoch 4/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 461.3604 - val_loss: 575.3680\n",
      "Epoch 5/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 439.4976 - val_loss: 550.8914\n",
      "Epoch 6/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 418.5588 - val_loss: 527.3967\n",
      "Epoch 7/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 398.8979 - val_loss: 505.5241\n",
      "Epoch 8/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 380.5332 - val_loss: 484.8098\n",
      "Epoch 9/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 363.6203 - val_loss: 465.5681\n",
      "Epoch 10/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 347.9818 - val_loss: 447.3117\n",
      "Epoch 11/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 332.7067 - val_loss: 430.3891\n",
      "Epoch 12/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 318.8333 - val_loss: 414.5621\n",
      "Epoch 13/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 306.1509 - val_loss: 399.8210\n",
      "Epoch 14/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 294.3271 - val_loss: 386.4906\n",
      "Epoch 15/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 283.6627 - val_loss: 373.8731\n",
      "Epoch 16/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 273.4754 - val_loss: 361.9947\n",
      "Epoch 17/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 264.3526 - val_loss: 351.0950\n",
      "Epoch 18/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 255.8165 - val_loss: 341.0453\n",
      "Epoch 19/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 248.0407 - val_loss: 331.7808\n",
      "Epoch 20/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 240.8550 - val_loss: 323.2953\n",
      "Epoch 21/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 234.3990 - val_loss: 315.4232\n",
      "Epoch 22/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 228.1833 - val_loss: 308.0440\n",
      "Epoch 23/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 222.5268 - val_loss: 301.0767\n",
      "Epoch 24/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 217.3526 - val_loss: 294.5414\n",
      "Epoch 25/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 212.2670 - val_loss: 288.1984\n",
      "Epoch 26/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 207.4803 - val_loss: 282.2367\n",
      "Epoch 27/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 203.1921 - val_loss: 276.8268\n",
      "Epoch 28/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 199.0959 - val_loss: 271.6372\n",
      "Epoch 29/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 195.3337 - val_loss: 266.6352\n",
      "Epoch 30/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 191.7271 - val_loss: 261.7074\n",
      "Epoch 31/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 188.2140 - val_loss: 257.0570\n",
      "Epoch 32/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 184.8806 - val_loss: 252.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 181.8437 - val_loss: 248.2156\n",
      "Epoch 34/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 178.7975 - val_loss: 244.0107\n",
      "Epoch 35/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 175.8580 - val_loss: 240.0605\n",
      "Epoch 36/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 173.1965 - val_loss: 236.2741\n",
      "Epoch 37/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 170.5627 - val_loss: 232.6703\n",
      "Epoch 38/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 168.1807 - val_loss: 229.1220\n",
      "Epoch 39/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 165.7327 - val_loss: 225.6536\n",
      "Epoch 40/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 163.5462 - val_loss: 222.4934\n",
      "Epoch 41/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 161.4010 - val_loss: 219.3914\n",
      "Epoch 42/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 159.3366 - val_loss: 216.3843\n",
      "Epoch 43/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 157.2870 - val_loss: 213.4017\n",
      "Epoch 44/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 155.4191 - val_loss: 210.5740\n",
      "Epoch 45/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 153.5637 - val_loss: 207.8340\n",
      "Epoch 46/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 151.6822 - val_loss: 205.2251\n",
      "Epoch 47/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 149.9950 - val_loss: 202.6006\n",
      "Epoch 48/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 148.2764 - val_loss: 200.0415\n",
      "Epoch 49/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 146.5629 - val_loss: 197.5438\n",
      "Epoch 50/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 144.9095 - val_loss: 195.1672\n",
      "Epoch 51/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 143.3393 - val_loss: 192.9010\n",
      "Epoch 52/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 141.7638 - val_loss: 190.6397\n",
      "Epoch 53/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 140.2580 - val_loss: 188.5504\n",
      "Epoch 54/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 138.7241 - val_loss: 186.3759\n",
      "Epoch 55/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 137.3025 - val_loss: 184.4102\n",
      "Epoch 56/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 135.8034 - val_loss: 182.4080\n",
      "Epoch 57/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 134.3158 - val_loss: 180.4314\n",
      "Epoch 58/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 132.9148 - val_loss: 178.5281\n",
      "Epoch 59/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 131.5034 - val_loss: 176.6524\n",
      "Epoch 60/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 130.1730 - val_loss: 174.8743\n",
      "Epoch 61/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 128.8679 - val_loss: 173.1680\n",
      "Epoch 62/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 127.5353 - val_loss: 171.3993\n",
      "Epoch 63/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 126.2698 - val_loss: 169.8126\n",
      "Epoch 64/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 125.0159 - val_loss: 168.1859\n",
      "Epoch 65/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 123.8418 - val_loss: 166.6240\n",
      "Epoch 66/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 122.6355 - val_loss: 165.1436\n",
      "Epoch 67/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 121.4785 - val_loss: 163.7008\n",
      "Epoch 68/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 120.3416 - val_loss: 162.2538\n",
      "Epoch 69/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 119.1663 - val_loss: 160.8638\n",
      "Epoch 70/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 117.9578 - val_loss: 159.4120\n",
      "Epoch 71/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 116.7666 - val_loss: 158.0701\n",
      "Epoch 72/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 115.6285 - val_loss: 156.7629\n",
      "Epoch 73/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 114.5338 - val_loss: 155.4131\n",
      "Epoch 74/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 113.4609 - val_loss: 154.1195\n",
      "Epoch 75/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 112.3891 - val_loss: 152.8633\n",
      "Epoch 76/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 111.3350 - val_loss: 151.6534\n",
      "Epoch 77/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 110.2751 - val_loss: 150.3964\n",
      "Epoch 78/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 109.2499 - val_loss: 149.2290\n",
      "Epoch 79/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 108.2676 - val_loss: 148.0805\n",
      "Epoch 80/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 107.3536 - val_loss: 146.9464\n",
      "Epoch 81/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 106.4598 - val_loss: 145.8659\n",
      "Epoch 82/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 105.5770 - val_loss: 144.9039\n",
      "Epoch 83/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.8331 - val_loss: 143.9177\n",
      "Epoch 84/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 104.0347 - val_loss: 142.9899\n",
      "Epoch 85/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 103.2752 - val_loss: 142.0244\n",
      "Epoch 86/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 102.5249 - val_loss: 141.2075\n",
      "Epoch 87/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.8453 - val_loss: 140.3897\n",
      "Epoch 88/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 101.2053 - val_loss: 139.6046\n",
      "Epoch 89/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 100.5479 - val_loss: 138.8546\n",
      "Epoch 90/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.9347 - val_loss: 138.2198\n",
      "Epoch 91/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 99.3961 - val_loss: 137.5433\n",
      "Epoch 92/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.8323 - val_loss: 136.8953\n",
      "Epoch 93/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 98.2875 - val_loss: 136.2134\n",
      "Epoch 94/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.7718 - val_loss: 135.5768\n",
      "Epoch 95/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 97.2622 - val_loss: 134.9578\n",
      "Epoch 96/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.7915 - val_loss: 134.3810\n",
      "Epoch 97/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 96.3204 - val_loss: 133.8096\n",
      "Epoch 98/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.8832 - val_loss: 133.3036\n",
      "Epoch 99/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.4811 - val_loss: 132.8050\n",
      "Epoch 100/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 95.0470 - val_loss: 132.2935\n",
      "Epoch 101/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.6118 - val_loss: 131.8075\n",
      "Epoch 102/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 94.2167 - val_loss: 131.3090\n",
      "Epoch 103/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.8354 - val_loss: 130.8618\n",
      "Epoch 104/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.4393 - val_loss: 130.3843\n",
      "Epoch 105/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 93.0393 - val_loss: 129.9312\n",
      "Epoch 106/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.6690 - val_loss: 129.4684\n",
      "Epoch 107/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 92.3140 - val_loss: 129.0605\n",
      "Epoch 108/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.9999 - val_loss: 128.6271\n",
      "Epoch 109/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.6927 - val_loss: 128.2179\n",
      "Epoch 110/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.3615 - val_loss: 127.8173\n",
      "Epoch 111/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 91.0634 - val_loss: 127.4406\n",
      "Epoch 112/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.7430 - val_loss: 127.0823\n",
      "Epoch 113/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.4520 - val_loss: 126.7051\n",
      "Epoch 114/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 90.1652 - val_loss: 126.3659\n",
      "Epoch 115/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.8815 - val_loss: 125.9650\n",
      "Epoch 116/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.5587 - val_loss: 125.5969\n",
      "Epoch 117/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.3023 - val_loss: 125.2958\n",
      "Epoch 118/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 89.0324 - val_loss: 124.9643\n",
      "Epoch 119/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.7543 - val_loss: 124.6053\n",
      "Epoch 120/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.4898 - val_loss: 124.2807\n",
      "Epoch 121/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 88.2373 - val_loss: 123.9964\n",
      "Epoch 122/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.9768 - val_loss: 123.7155\n",
      "Epoch 123/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.7663 - val_loss: 123.4483\n",
      "Epoch 124/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.5064 - val_loss: 123.1338\n",
      "Epoch 125/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.2791 - val_loss: 122.8726\n",
      "Epoch 126/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 87.0556 - val_loss: 122.6543\n",
      "Epoch 127/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.8381 - val_loss: 122.4338\n",
      "Epoch 128/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.6366 - val_loss: 122.1832\n",
      "Epoch 129/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.4047 - val_loss: 121.8957\n",
      "Epoch 130/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 86.1866 - val_loss: 121.6601\n",
      "Epoch 131/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.9694 - val_loss: 121.4143\n",
      "Epoch 132/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.7489 - val_loss: 121.2101\n",
      "Epoch 133/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.5401 - val_loss: 120.9727\n",
      "Epoch 134/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.3434 - val_loss: 120.7028\n",
      "Epoch 135/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 85.1314 - val_loss: 120.4819\n",
      "Epoch 136/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.9415 - val_loss: 120.2473\n",
      "Epoch 137/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.7572 - val_loss: 120.0552\n",
      "Epoch 138/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.5764 - val_loss: 119.8153\n",
      "Epoch 139/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.3879 - val_loss: 119.6016\n",
      "Epoch 140/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.2049 - val_loss: 119.4330\n",
      "Epoch 141/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 84.0238 - val_loss: 119.2373\n",
      "Epoch 142/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.8570 - val_loss: 119.0787\n",
      "Epoch 143/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.6922 - val_loss: 118.9265\n",
      "Epoch 144/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.5256 - val_loss: 118.7854\n",
      "Epoch 145/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.3617 - val_loss: 118.6280\n",
      "Epoch 146/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.1892 - val_loss: 118.4320\n",
      "Epoch 147/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 83.0218 - val_loss: 118.2957\n",
      "Epoch 148/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.8662 - val_loss: 118.1016\n",
      "Epoch 149/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.6954 - val_loss: 117.9276\n",
      "Epoch 150/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.5278 - val_loss: 117.7698\n",
      "Epoch 151/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.3674 - val_loss: 117.6322\n",
      "Epoch 152/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.2115 - val_loss: 117.4768\n",
      "Epoch 153/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 82.0645 - val_loss: 117.3150\n",
      "Epoch 154/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.9127 - val_loss: 117.1709\n",
      "Epoch 155/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.7748 - val_loss: 116.9968\n",
      "Epoch 156/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.6204 - val_loss: 116.8335\n",
      "Epoch 157/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.4726 - val_loss: 116.6841\n",
      "Epoch 158/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.3297 - val_loss: 116.5317\n",
      "Epoch 159/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.1926 - val_loss: 116.3740\n",
      "Epoch 160/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 81.0491 - val_loss: 116.2297\n",
      "Epoch 161/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.9371 - val_loss: 116.1038\n",
      "Epoch 162/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.8034 - val_loss: 116.0115\n",
      "Epoch 163/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.6717 - val_loss: 115.8823\n",
      "Epoch 164/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.5662 - val_loss: 115.7260\n",
      "Epoch 165/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.4447 - val_loss: 115.6427\n",
      "Epoch 166/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.3223 - val_loss: 115.5494\n",
      "Epoch 167/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.2128 - val_loss: 115.4423\n",
      "Epoch 168/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.1068 - val_loss: 115.3191\n",
      "Epoch 169/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 80.0013 - val_loss: 115.2086\n",
      "Epoch 170/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.8899 - val_loss: 115.0903\n",
      "Epoch 171/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.7865 - val_loss: 115.0209\n",
      "Epoch 172/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.6809 - val_loss: 114.8823\n",
      "Epoch 173/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.5822 - val_loss: 114.7928\n",
      "Epoch 174/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.4835 - val_loss: 114.6573\n",
      "Epoch 175/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3990 - val_loss: 114.5524\n",
      "Epoch 176/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.3105 - val_loss: 114.4902\n",
      "Epoch 177/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.2098 - val_loss: 114.3879\n",
      "Epoch 178/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.1223 - val_loss: 114.3019\n",
      "Epoch 179/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 79.0226 - val_loss: 114.2164\n",
      "Epoch 180/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.9333 - val_loss: 114.0966\n",
      "Epoch 181/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.8392 - val_loss: 114.0237\n",
      "Epoch 182/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.7508 - val_loss: 113.9620\n",
      "Epoch 183/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.6776 - val_loss: 113.8997\n",
      "Epoch 184/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.5977 - val_loss: 113.8305\n",
      "Epoch 185/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.5302 - val_loss: 113.7517\n",
      "Epoch 186/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.4618 - val_loss: 113.6971\n",
      "Epoch 187/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 0s 1ms/step - loss: 78.3821 - val_loss: 113.6476\n",
      "Epoch 188/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.3252 - val_loss: 113.5935\n",
      "Epoch 189/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.2428 - val_loss: 113.5148\n",
      "Epoch 190/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.1807 - val_loss: 113.4598\n",
      "Epoch 191/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.1219 - val_loss: 113.4064\n",
      "Epoch 192/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 78.0582 - val_loss: 113.3350\n",
      "Epoch 193/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.9833 - val_loss: 113.2761\n",
      "Epoch 194/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.9227 - val_loss: 113.2146\n",
      "Epoch 195/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.8902 - val_loss: 113.1978\n",
      "Epoch 196/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.8148 - val_loss: 113.1157\n",
      "Epoch 197/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.7580 - val_loss: 113.0602\n",
      "Epoch 198/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.7068 - val_loss: 112.9961\n",
      "Epoch 199/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6618 - val_loss: 112.9450\n",
      "Epoch 200/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.6018 - val_loss: 112.9052\n",
      "Epoch 201/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.5538 - val_loss: 112.8585\n",
      "Epoch 202/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4900 - val_loss: 112.8085\n",
      "Epoch 203/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.4502 - val_loss: 112.7653\n",
      "Epoch 204/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.3903 - val_loss: 112.6995\n",
      "Epoch 205/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.3498 - val_loss: 112.6507\n",
      "Epoch 206/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.3179 - val_loss: 112.6197\n",
      "Epoch 207/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 77.2624 - val_loss: 112.5146\n",
      "Epoch 208/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 77.2196 - val_loss: 112.5065\n",
      "Epoch 209/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.1520 - val_loss: 112.4685\n",
      "Epoch 210/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.1269 - val_loss: 112.4324\n",
      "Epoch 211/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0959 - val_loss: 112.4321\n",
      "Epoch 212/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0594 - val_loss: 112.3848\n",
      "Epoch 213/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 77.0060 - val_loss: 112.3555\n",
      "Epoch 214/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.9616 - val_loss: 112.3113\n",
      "Epoch 215/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.9178 - val_loss: 112.2840\n",
      "Epoch 216/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8624 - val_loss: 112.2421\n",
      "Epoch 217/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.8237 - val_loss: 112.1847\n",
      "Epoch 218/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.7721 - val_loss: 112.1237\n",
      "Epoch 219/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.7342 - val_loss: 112.0638\n",
      "Epoch 220/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.6843 - val_loss: 111.9885\n",
      "Epoch 221/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.6347 - val_loss: 111.9271\n",
      "Epoch 222/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.5716 - val_loss: 111.8886\n",
      "Epoch 223/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.5234 - val_loss: 111.8112\n",
      "Epoch 224/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4829 - val_loss: 111.7645\n",
      "Epoch 225/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4414 - val_loss: 111.6851\n",
      "Epoch 226/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.4068 - val_loss: 111.6531\n",
      "Epoch 227/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.3811 - val_loss: 111.5905\n",
      "Epoch 228/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.3154 - val_loss: 111.5378\n",
      "Epoch 229/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.2705 - val_loss: 111.5084\n",
      "Epoch 230/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.2257 - val_loss: 111.4745\n",
      "Epoch 231/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.1659 - val_loss: 111.4193\n",
      "Epoch 232/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.1472 - val_loss: 111.3679\n",
      "Epoch 233/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.1133 - val_loss: 111.3174\n",
      "Epoch 234/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0968 - val_loss: 111.2805\n",
      "Epoch 235/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0412 - val_loss: 111.2322\n",
      "Epoch 236/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 76.0255 - val_loss: 111.2057\n",
      "Epoch 237/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.9824 - val_loss: 111.1341\n",
      "Epoch 238/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.9327 - val_loss: 111.0325\n",
      "Epoch 239/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8988 - val_loss: 110.9974\n",
      "Epoch 240/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8465 - val_loss: 110.9223\n",
      "Epoch 241/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.8195 - val_loss: 110.8665\n",
      "Epoch 242/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.7766 - val_loss: 110.7924\n",
      "Epoch 243/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.7168 - val_loss: 110.7258\n",
      "Epoch 244/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6916 - val_loss: 110.6423\n",
      "Epoch 245/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6516 - val_loss: 110.5604\n",
      "Epoch 246/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6348 - val_loss: 110.5393\n",
      "Epoch 247/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.6019 - val_loss: 110.4399\n",
      "Epoch 248/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.5597 - val_loss: 110.3994\n",
      "Epoch 249/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.5426 - val_loss: 110.3338\n",
      "Epoch 250/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4994 - val_loss: 110.2069\n",
      "Epoch 251/300\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 75.4731 - val_loss: 110.1315\n",
      "Epoch 252/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4367 - val_loss: 110.0789\n",
      "Epoch 253/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.4150 - val_loss: 110.0744\n",
      "Epoch 254/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3951 - val_loss: 110.0318\n",
      "Epoch 255/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3745 - val_loss: 109.9897\n",
      "Epoch 256/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.3142 - val_loss: 109.8995\n",
      "Epoch 257/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.2908 - val_loss: 109.7812\n",
      "Epoch 258/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.2579 - val_loss: 109.7374\n",
      "Epoch 259/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.2358 - val_loss: 109.7424\n",
      "Epoch 260/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1966 - val_loss: 109.6557\n",
      "Epoch 261/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1750 - val_loss: 109.6068\n",
      "Epoch 262/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1235 - val_loss: 109.5520\n",
      "Epoch 263/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.1101 - val_loss: 109.4497\n",
      "Epoch 264/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0847 - val_loss: 109.3946\n",
      "Epoch 265/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0383 - val_loss: 109.3648\n",
      "Epoch 266/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0405 - val_loss: 109.3261\n",
      "Epoch 267/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 75.0052 - val_loss: 109.2460\n",
      "Epoch 268/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.9897 - val_loss: 109.2313\n",
      "Epoch 269/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.9298 - val_loss: 109.2308\n",
      "Epoch 270/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.9429 - val_loss: 109.1691\n",
      "Epoch 271/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8965 - val_loss: 109.0981\n",
      "Epoch 272/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8620 - val_loss: 109.0645\n",
      "Epoch 273/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8371 - val_loss: 109.0223\n",
      "Epoch 274/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.8019 - val_loss: 108.9845\n",
      "Epoch 275/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.7741 - val_loss: 108.9383\n",
      "Epoch 276/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.7355 - val_loss: 108.8451\n",
      "Epoch 277/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.6945 - val_loss: 108.7841\n",
      "Epoch 278/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.6481 - val_loss: 108.7659\n",
      "Epoch 279/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.6284 - val_loss: 108.6954\n",
      "Epoch 280/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.5923 - val_loss: 108.6072\n",
      "Epoch 281/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.5415 - val_loss: 108.5448\n",
      "Epoch 282/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.5085 - val_loss: 108.5365\n",
      "Epoch 283/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.4610 - val_loss: 108.4818\n",
      "Epoch 284/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.3936 - val_loss: 108.3931\n",
      "Epoch 285/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.3417 - val_loss: 108.2606\n",
      "Epoch 286/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.2846 - val_loss: 108.2366\n",
      "Epoch 287/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.2032 - val_loss: 108.1578\n",
      "Epoch 288/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.1711 - val_loss: 108.1084\n",
      "Epoch 289/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.1049 - val_loss: 108.0201\n",
      "Epoch 290/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 74.0414 - val_loss: 107.9464\n",
      "Epoch 291/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.9658 - val_loss: 107.8136\n",
      "Epoch 292/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.8751 - val_loss: 107.6936\n",
      "Epoch 293/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7924 - val_loss: 107.5763\n",
      "Epoch 294/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.7005 - val_loss: 107.4529\n",
      "Epoch 295/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 73.5972 - val_loss: 107.3387\n",
      "Epoch 296/300\n",
      "303/303 [==============================] - 0s 2ms/step - loss: 73.4772 - val_loss: 107.2229\n",
      "Epoch 297/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.3907 - val_loss: 107.1719\n",
      "Epoch 298/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.3038 - val_loss: 107.0438\n",
      "Epoch 299/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.1905 - val_loss: 106.9143\n",
      "Epoch 300/300\n",
      "303/303 [==============================] - 0s 1ms/step - loss: 73.0716 - val_loss: 106.8089\n"
     ]
    }
   ],
   "source": [
    "listDA_L = []; listDV_L = []\n",
    "for i in range(4):\n",
    "    DV = DA[i*NV:(i+1)*NV]; Dv = Da[i*NV:(i+1)*NV];\n",
    "    DAp = np.concatenate([DA[:i*NV],DA[(i+1)*NV:]],axis=0)\n",
    "    Dap = np.concatenate([Da[:i*NV],Da[(i+1)*NV:]],axis=0)\n",
    "    modèle,h = Apprentissage(DAp,Dap)\n",
    "    listDA_L.append(h['loss']); listDV_L.append(h['val_loss'])\n",
    "\n",
    "DA_L = [np.mean([y[i] for y in listDA_L]) for i in range(nb_époques)]\n",
    "DV_L = [np.mean([y[i] for y in listDV_L]) for i in range(nb_époques)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dfbd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  1 loss :  566.4640350341797 val_loss :  555.6436462402344\n",
      "iteration :  2 loss :  543.4768142700195 val_loss :  531.5094223022461\n",
      "iteration :  3 loss :  519.0196228027344 val_loss :  507.6222686767578\n",
      "iteration :  4 loss :  495.5177917480469 val_loss :  484.72010040283203\n",
      "iteration :  5 loss :  473.01236724853516 val_loss :  462.9796600341797\n",
      "iteration :  6 loss :  451.7186279296875 val_loss :  442.2658996582031\n",
      "iteration :  7 loss :  431.29518127441406 val_loss :  422.74063873291016\n",
      "iteration :  8 loss :  412.1791534423828 val_loss :  404.4472198486328\n",
      "iteration :  9 loss :  394.47147369384766 val_loss :  387.27622985839844\n",
      "iteration :  10 loss :  377.96807861328125 val_loss :  371.2413024902344\n",
      "iteration :  11 loss :  362.27164459228516 val_loss :  356.38587951660156\n",
      "iteration :  12 loss :  347.79361724853516 val_loss :  342.5811080932617\n",
      "iteration :  13 loss :  334.28797912597656 val_loss :  329.614990234375\n",
      "iteration :  14 loss :  321.93543243408203 val_loss :  317.7428436279297\n",
      "iteration :  15 loss :  310.3437957763672 val_loss :  306.75170135498047\n",
      "iteration :  16 loss :  299.65992736816406 val_loss :  296.5709533691406\n",
      "iteration :  17 loss :  289.9772033691406 val_loss :  287.2105484008789\n",
      "iteration :  18 loss :  280.85887908935547 val_loss :  278.5783805847168\n",
      "iteration :  19 loss :  272.4555015563965 val_loss :  270.572566986084\n",
      "iteration :  20 loss :  264.76518630981445 val_loss :  263.2585220336914\n",
      "iteration :  21 loss :  257.65221786499023 val_loss :  256.479305267334\n",
      "iteration :  22 loss :  250.9964370727539 val_loss :  250.12178802490234\n",
      "iteration :  23 loss :  244.6282196044922 val_loss :  244.18356323242188\n",
      "iteration :  24 loss :  238.89971542358398 val_loss :  238.65789413452148\n",
      "iteration :  25 loss :  233.24021911621094 val_loss :  233.36911392211914\n",
      "iteration :  26 loss :  227.98702239990234 val_loss :  228.42321395874023\n",
      "iteration :  27 loss :  223.11767959594727 val_loss :  223.79584503173828\n",
      "iteration :  28 loss :  218.50156784057617 val_loss :  219.41468048095703\n",
      "iteration :  29 loss :  214.0435905456543 val_loss :  215.2233772277832\n",
      "iteration :  30 loss :  209.8892936706543 val_loss :  211.20917892456055\n",
      "iteration :  31 loss :  205.82363510131836 val_loss :  207.40654373168945\n",
      "iteration :  32 loss :  201.99499893188477 val_loss :  203.78952026367188\n",
      "iteration :  33 loss :  198.33538055419922 val_loss :  200.32313919067383\n",
      "iteration :  34 loss :  194.8470344543457 val_loss :  197.01016998291016\n",
      "iteration :  35 loss :  191.52106475830078 val_loss :  193.9035758972168\n",
      "iteration :  36 loss :  188.37080001831055 val_loss :  190.86903381347656\n",
      "iteration :  37 loss :  185.2998390197754 val_loss :  187.9844512939453\n",
      "iteration :  38 loss :  182.37282180786133 val_loss :  185.16518783569336\n",
      "iteration :  39 loss :  179.55054092407227 val_loss :  182.4992904663086\n",
      "iteration :  40 loss :  176.89064407348633 val_loss :  179.9576530456543\n",
      "iteration :  41 loss :  174.2954864501953 val_loss :  177.47891235351562\n",
      "iteration :  42 loss :  171.80159759521484 val_loss :  175.09263610839844\n",
      "iteration :  43 loss :  169.39474868774414 val_loss :  172.77276992797852\n",
      "iteration :  44 loss :  167.10338973999023 val_loss :  170.55643844604492\n",
      "iteration :  45 loss :  164.86310195922852 val_loss :  168.38415908813477\n",
      "iteration :  46 loss :  162.68622207641602 val_loss :  166.30015182495117\n",
      "iteration :  47 loss :  160.57879257202148 val_loss :  164.23807525634766\n",
      "iteration :  48 loss :  158.5704574584961 val_loss :  162.23752975463867\n",
      "iteration :  49 loss :  156.56023025512695 val_loss :  160.28899383544922\n",
      "iteration :  50 loss :  154.65276336669922 val_loss :  158.3842315673828\n",
      "iteration :  51 loss :  152.8091278076172 val_loss :  156.54875946044922\n",
      "iteration :  52 loss :  151.02067947387695 val_loss :  154.7555103302002\n",
      "iteration :  53 loss :  149.31032943725586 val_loss :  153.04657363891602\n",
      "iteration :  54 loss :  147.63032150268555 val_loss :  151.3261260986328\n",
      "iteration :  55 loss :  145.96559524536133 val_loss :  149.682767868042\n",
      "iteration :  56 loss :  144.37809371948242 val_loss :  148.06399154663086\n",
      "iteration :  57 loss :  142.8072509765625 val_loss :  146.4393367767334\n",
      "iteration :  58 loss :  141.28695106506348 val_loss :  144.87372016906738\n",
      "iteration :  59 loss :  139.81180572509766 val_loss :  143.28936958312988\n",
      "iteration :  60 loss :  138.38125228881836 val_loss :  141.73685455322266\n",
      "iteration :  61 loss :  136.96417427062988 val_loss :  140.24173545837402\n",
      "iteration :  62 loss :  135.62715911865234 val_loss :  138.78307914733887\n",
      "iteration :  63 loss :  134.30380630493164 val_loss :  137.383394241333\n",
      "iteration :  64 loss :  133.0395278930664 val_loss :  136.0053367614746\n",
      "iteration :  65 loss :  131.7770538330078 val_loss :  134.67980194091797\n",
      "iteration :  66 loss :  130.5411148071289 val_loss :  133.4169406890869\n",
      "iteration :  67 loss :  129.36943435668945 val_loss :  132.1667366027832\n",
      "iteration :  68 loss :  128.21468544006348 val_loss :  130.96540069580078\n",
      "iteration :  69 loss :  127.07250595092773 val_loss :  129.79999923706055\n",
      "iteration :  70 loss :  125.9521713256836 val_loss :  128.65017700195312\n",
      "iteration :  71 loss :  124.87060928344727 val_loss :  127.58055686950684\n",
      "iteration :  72 loss :  123.81757926940918 val_loss :  126.51608085632324\n",
      "iteration :  73 loss :  122.79757308959961 val_loss :  125.45296287536621\n",
      "iteration :  74 loss :  121.79549980163574 val_loss :  124.45571708679199\n",
      "iteration :  75 loss :  120.80327224731445 val_loss :  123.47485733032227\n",
      "iteration :  76 loss :  119.81000900268555 val_loss :  122.51422119140625\n",
      "iteration :  77 loss :  118.80821228027344 val_loss :  121.57475662231445\n",
      "iteration :  78 loss :  117.8376522064209 val_loss :  120.63580703735352\n",
      "iteration :  79 loss :  116.91221809387207 val_loss :  119.75741577148438\n",
      "iteration :  80 loss :  115.9732780456543 val_loss :  118.86859512329102\n",
      "iteration :  81 loss :  115.09812545776367 val_loss :  118.0464096069336\n",
      "iteration :  82 loss :  114.20968055725098 val_loss :  117.23581314086914\n",
      "iteration :  83 loss :  113.3831787109375 val_loss :  116.44526863098145\n",
      "iteration :  84 loss :  112.50929641723633 val_loss :  115.68362045288086\n",
      "iteration :  85 loss :  111.69476890563965 val_loss :  114.92123413085938\n",
      "iteration :  86 loss :  110.90615272521973 val_loss :  114.19361686706543\n",
      "iteration :  87 loss :  110.15929412841797 val_loss :  113.49849510192871\n",
      "iteration :  88 loss :  109.41263580322266 val_loss :  112.79829978942871\n",
      "iteration :  89 loss :  108.6799087524414 val_loss :  112.13369750976562\n",
      "iteration :  90 loss :  107.9725513458252 val_loss :  111.50227928161621\n",
      "iteration :  91 loss :  107.30936813354492 val_loss :  110.8774356842041\n",
      "iteration :  92 loss :  106.65429878234863 val_loss :  110.27727127075195\n",
      "iteration :  93 loss :  106.03227233886719 val_loss :  109.68613815307617\n",
      "iteration :  94 loss :  105.43950080871582 val_loss :  109.12590789794922\n",
      "iteration :  95 loss :  104.83363342285156 val_loss :  108.57892227172852\n",
      "iteration :  96 loss :  104.29414749145508 val_loss :  108.0490951538086\n",
      "iteration :  97 loss :  103.74172592163086 val_loss :  107.56132316589355\n",
      "iteration :  98 loss :  103.23556518554688 val_loss :  107.0910758972168\n",
      "iteration :  99 loss :  102.71428489685059 val_loss :  106.61969757080078\n",
      "iteration :  100 loss :  102.21563529968262 val_loss :  106.15307998657227\n",
      "iteration :  101 loss :  101.7392578125 val_loss :  105.71039199829102\n",
      "iteration :  102 loss :  101.24200439453125 val_loss :  105.27091789245605\n",
      "iteration :  103 loss :  100.79711532592773 val_loss :  104.85884094238281\n",
      "iteration :  104 loss :  100.36253356933594 val_loss :  104.44678688049316\n",
      "iteration :  105 loss :  99.91484451293945 val_loss :  104.04088401794434\n",
      "iteration :  106 loss :  99.49214553833008 val_loss :  103.64462852478027\n",
      "iteration :  107 loss :  99.0784683227539 val_loss :  103.28110313415527\n",
      "iteration :  108 loss :  98.69045066833496 val_loss :  102.88984489440918\n",
      "iteration :  109 loss :  98.29133224487305 val_loss :  102.52606201171875\n",
      "iteration :  110 loss :  97.90662574768066 val_loss :  102.17475891113281\n",
      "iteration :  111 loss :  97.54578971862793 val_loss :  101.82589912414551\n",
      "iteration :  112 loss :  97.17746353149414 val_loss :  101.5043716430664\n",
      "iteration :  113 loss :  96.82147789001465 val_loss :  101.17362976074219\n",
      "iteration :  114 loss :  96.48723602294922 val_loss :  100.85862350463867\n",
      "iteration :  115 loss :  96.15835189819336 val_loss :  100.54413604736328\n",
      "iteration :  116 loss :  95.82326316833496 val_loss :  100.22990798950195\n",
      "iteration :  117 loss :  95.4971923828125 val_loss :  99.9337215423584\n",
      "iteration :  118 loss :  95.1801929473877 val_loss :  99.63594245910645\n",
      "iteration :  119 loss :  94.86450958251953 val_loss :  99.34146308898926\n",
      "iteration :  120 loss :  94.56284523010254 val_loss :  99.05201721191406\n",
      "iteration :  121 loss :  94.27435111999512 val_loss :  98.79683303833008\n",
      "iteration :  122 loss :  93.9853343963623 val_loss :  98.53416061401367\n",
      "iteration :  123 loss :  93.7077579498291 val_loss :  98.28557586669922\n",
      "iteration :  124 loss :  93.42716407775879 val_loss :  98.03089332580566\n",
      "iteration :  125 loss :  93.17529487609863 val_loss :  97.78826904296875\n",
      "iteration :  126 loss :  92.92350387573242 val_loss :  97.55400085449219\n",
      "iteration :  127 loss :  92.66737365722656 val_loss :  97.31772232055664\n",
      "iteration :  128 loss :  92.41769790649414 val_loss :  97.09214878082275\n",
      "iteration :  129 loss :  92.17494010925293 val_loss :  96.86038112640381\n",
      "iteration :  130 loss :  91.93442344665527 val_loss :  96.63030242919922\n",
      "iteration :  131 loss :  91.70351982116699 val_loss :  96.39756107330322\n",
      "iteration :  132 loss :  91.47266960144043 val_loss :  96.19732189178467\n",
      "iteration :  133 loss :  91.25317192077637 val_loss :  95.981614112854\n",
      "iteration :  134 loss :  91.0305118560791 val_loss :  95.76833534240723\n",
      "iteration :  135 loss :  90.81552314758301 val_loss :  95.5589771270752\n",
      "iteration :  136 loss :  90.6141586303711 val_loss :  95.36752223968506\n",
      "iteration :  137 loss :  90.41691017150879 val_loss :  95.17756843566895\n",
      "iteration :  138 loss :  90.21453285217285 val_loss :  94.97314262390137\n",
      "iteration :  139 loss :  90.01737594604492 val_loss :  94.76516437530518\n",
      "iteration :  140 loss :  89.80949783325195 val_loss :  94.58569812774658\n",
      "iteration :  141 loss :  89.62693977355957 val_loss :  94.39076328277588\n",
      "iteration :  142 loss :  89.43649101257324 val_loss :  94.21052074432373\n",
      "iteration :  143 loss :  89.25835418701172 val_loss :  94.03777313232422\n",
      "iteration :  144 loss :  89.08043670654297 val_loss :  93.86087417602539\n",
      "iteration :  145 loss :  88.90872001647949 val_loss :  93.69023704528809\n",
      "iteration :  146 loss :  88.73054504394531 val_loss :  93.51279258728027\n",
      "iteration :  147 loss :  88.55985069274902 val_loss :  93.35223770141602\n",
      "iteration :  148 loss :  88.39043045043945 val_loss :  93.18439865112305\n",
      "iteration :  149 loss :  88.20267295837402 val_loss :  93.00934982299805\n",
      "iteration :  150 loss :  88.03060150146484 val_loss :  92.86014175415039\n",
      "iteration :  151 loss :  87.86608123779297 val_loss :  92.71954536437988\n",
      "iteration :  152 loss :  87.70805358886719 val_loss :  92.56838798522949\n",
      "iteration :  153 loss :  87.55801010131836 val_loss :  92.4053144454956\n",
      "iteration :  154 loss :  87.39528274536133 val_loss :  92.2566556930542\n",
      "iteration :  155 loss :  87.23397636413574 val_loss :  92.09199237823486\n",
      "iteration :  156 loss :  87.0875415802002 val_loss :  91.9534158706665\n",
      "iteration :  157 loss :  86.92562484741211 val_loss :  91.81165409088135\n",
      "iteration :  158 loss :  86.77146911621094 val_loss :  91.67743587493896\n",
      "iteration :  159 loss :  86.61878395080566 val_loss :  91.54942321777344\n",
      "iteration :  160 loss :  86.47296524047852 val_loss :  91.41245174407959\n",
      "iteration :  161 loss :  86.33831596374512 val_loss :  91.27589416503906\n",
      "iteration :  162 loss :  86.19278144836426 val_loss :  91.14808940887451\n",
      "iteration :  163 loss :  86.05179595947266 val_loss :  91.02549362182617\n",
      "iteration :  164 loss :  85.90918350219727 val_loss :  90.89593315124512\n",
      "iteration :  165 loss :  85.76503944396973 val_loss :  90.77299308776855\n",
      "iteration :  166 loss :  85.63733100891113 val_loss :  90.65372276306152\n",
      "iteration :  167 loss :  85.50746726989746 val_loss :  90.51280784606934\n",
      "iteration :  168 loss :  85.37469673156738 val_loss :  90.37311744689941\n",
      "iteration :  169 loss :  85.2382698059082 val_loss :  90.2245044708252\n",
      "iteration :  170 loss :  85.10767364501953 val_loss :  90.09566116333008\n",
      "iteration :  171 loss :  84.9919376373291 val_loss :  89.98288249969482\n",
      "iteration :  172 loss :  84.8572883605957 val_loss :  89.83353233337402\n",
      "iteration :  173 loss :  84.71939086914062 val_loss :  89.71975803375244\n",
      "iteration :  174 loss :  84.5893383026123 val_loss :  89.59972095489502\n",
      "iteration :  175 loss :  84.46328926086426 val_loss :  89.46777534484863\n",
      "iteration :  176 loss :  84.34477233886719 val_loss :  89.35558319091797\n",
      "iteration :  177 loss :  84.21923637390137 val_loss :  89.24264907836914\n",
      "iteration :  178 loss :  84.09425926208496 val_loss :  89.11776065826416\n",
      "iteration :  179 loss :  83.96862030029297 val_loss :  88.99224853515625\n",
      "iteration :  180 loss :  83.84076118469238 val_loss :  88.86610507965088\n",
      "iteration :  181 loss :  83.7218132019043 val_loss :  88.75681972503662\n",
      "iteration :  182 loss :  83.60549354553223 val_loss :  88.61878681182861\n",
      "iteration :  183 loss :  83.48172760009766 val_loss :  88.50356578826904\n",
      "iteration :  184 loss :  83.36416435241699 val_loss :  88.37178134918213\n",
      "iteration :  185 loss :  83.26252555847168 val_loss :  88.24743270874023\n",
      "iteration :  186 loss :  83.15161323547363 val_loss :  88.13489532470703\n",
      "iteration :  187 loss :  83.03925895690918 val_loss :  88.01235389709473\n",
      "iteration :  188 loss :  82.92227935791016 val_loss :  87.89525508880615\n",
      "iteration :  189 loss :  82.80133056640625 val_loss :  87.7792911529541\n",
      "iteration :  190 loss :  82.7165298461914 val_loss :  87.66723155975342\n",
      "iteration :  191 loss :  82.61026191711426 val_loss :  87.56816005706787\n",
      "iteration :  192 loss :  82.5013427734375 val_loss :  87.45358753204346\n",
      "iteration :  193 loss :  82.3913345336914 val_loss :  87.3460464477539\n",
      "iteration :  194 loss :  82.29021835327148 val_loss :  87.22842311859131\n",
      "iteration :  195 loss :  82.19573211669922 val_loss :  87.1474609375\n",
      "iteration :  196 loss :  82.0915412902832 val_loss :  87.03401374816895\n",
      "iteration :  197 loss :  81.97903442382812 val_loss :  86.93263149261475\n",
      "iteration :  198 loss :  81.8818130493164 val_loss :  86.83729457855225\n",
      "iteration :  199 loss :  81.79144096374512 val_loss :  86.74963188171387\n",
      "iteration :  200 loss :  81.70089721679688 val_loss :  86.63977527618408\n",
      "iteration :  201 loss :  81.6024341583252 val_loss :  86.5374641418457\n",
      "iteration :  202 loss :  81.49784851074219 val_loss :  86.44306373596191\n",
      "iteration :  203 loss :  81.41023063659668 val_loss :  86.33804702758789\n",
      "iteration :  204 loss :  81.32106590270996 val_loss :  86.2318811416626\n",
      "iteration :  205 loss :  81.23479843139648 val_loss :  86.11679649353027\n",
      "iteration :  206 loss :  81.15276718139648 val_loss :  86.01107978820801\n",
      "iteration :  207 loss :  81.06377410888672 val_loss :  85.89988136291504\n",
      "iteration :  208 loss :  80.96833801269531 val_loss :  85.80966758728027\n",
      "iteration :  209 loss :  80.86450576782227 val_loss :  85.70763969421387\n",
      "iteration :  210 loss :  80.77899742126465 val_loss :  85.62323665618896\n",
      "iteration :  211 loss :  80.67716407775879 val_loss :  85.54729843139648\n",
      "iteration :  212 loss :  80.58509063720703 val_loss :  85.4768238067627\n",
      "iteration :  213 loss :  80.50227546691895 val_loss :  85.39062881469727\n",
      "iteration :  214 loss :  80.41588020324707 val_loss :  85.30190658569336\n",
      "iteration :  215 loss :  80.32129096984863 val_loss :  85.22621059417725\n",
      "iteration :  216 loss :  80.2374496459961 val_loss :  85.13527393341064\n",
      "iteration :  217 loss :  80.15377044677734 val_loss :  85.03929328918457\n",
      "iteration :  218 loss :  80.05034065246582 val_loss :  84.94679260253906\n",
      "iteration :  219 loss :  79.97338485717773 val_loss :  84.86133575439453\n",
      "iteration :  220 loss :  79.88385963439941 val_loss :  84.78454971313477\n",
      "iteration :  221 loss :  79.8000717163086 val_loss :  84.70054817199707\n",
      "iteration :  222 loss :  79.70695877075195 val_loss :  84.63846778869629\n",
      "iteration :  223 loss :  79.6423397064209 val_loss :  84.56073570251465\n",
      "iteration :  224 loss :  79.57990455627441 val_loss :  84.47690963745117\n",
      "iteration :  225 loss :  79.49427795410156 val_loss :  84.37786865234375\n",
      "iteration :  226 loss :  79.41407585144043 val_loss :  84.3126049041748\n",
      "iteration :  227 loss :  79.33525657653809 val_loss :  84.22623252868652\n",
      "iteration :  228 loss :  79.2669506072998 val_loss :  84.1614351272583\n",
      "iteration :  229 loss :  79.18746566772461 val_loss :  84.10040283203125\n",
      "iteration :  230 loss :  79.10805320739746 val_loss :  84.04287910461426\n",
      "iteration :  231 loss :  79.05015754699707 val_loss :  83.95107650756836\n",
      "iteration :  232 loss :  78.97370910644531 val_loss :  83.8748722076416\n",
      "iteration :  233 loss :  78.90054130554199 val_loss :  83.78918075561523\n",
      "iteration :  234 loss :  78.84264183044434 val_loss :  83.7278060913086\n",
      "iteration :  235 loss :  78.77430534362793 val_loss :  83.6630048751831\n",
      "iteration :  236 loss :  78.71061897277832 val_loss :  83.60842418670654\n",
      "iteration :  237 loss :  78.65221214294434 val_loss :  83.5252161026001\n",
      "iteration :  238 loss :  78.58380508422852 val_loss :  83.4394178390503\n",
      "iteration :  239 loss :  78.51040077209473 val_loss :  83.3595085144043\n",
      "iteration :  240 loss :  78.44267463684082 val_loss :  83.28981399536133\n",
      "iteration :  241 loss :  78.35683250427246 val_loss :  83.22504615783691\n",
      "iteration :  242 loss :  78.28561210632324 val_loss :  83.14689064025879\n",
      "iteration :  243 loss :  78.22237586975098 val_loss :  83.0779857635498\n",
      "iteration :  244 loss :  78.16652870178223 val_loss :  82.98355007171631\n",
      "iteration :  245 loss :  78.1002140045166 val_loss :  82.90085697174072\n",
      "iteration :  246 loss :  78.04668617248535 val_loss :  82.8340253829956\n",
      "iteration :  247 loss :  77.98050498962402 val_loss :  82.73849773406982\n",
      "iteration :  248 loss :  77.90800285339355 val_loss :  82.64345359802246\n",
      "iteration :  249 loss :  77.85185813903809 val_loss :  82.56511878967285\n",
      "iteration :  250 loss :  77.78924179077148 val_loss :  82.4714241027832\n",
      "iteration :  251 loss :  77.72665596008301 val_loss :  82.38222789764404\n",
      "iteration :  252 loss :  77.67556762695312 val_loss :  82.3185567855835\n",
      "iteration :  253 loss :  77.60318756103516 val_loss :  82.25546836853027\n",
      "iteration :  254 loss :  77.53018760681152 val_loss :  82.20113372802734\n",
      "iteration :  255 loss :  77.46790313720703 val_loss :  82.13086318969727\n",
      "iteration :  256 loss :  77.41356658935547 val_loss :  82.05079650878906\n",
      "iteration :  257 loss :  77.3345890045166 val_loss :  81.96178436279297\n",
      "iteration :  258 loss :  77.25700759887695 val_loss :  81.87192726135254\n",
      "iteration :  259 loss :  77.19361305236816 val_loss :  81.79550170898438\n",
      "iteration :  260 loss :  77.1030216217041 val_loss :  81.70528507232666\n",
      "iteration :  261 loss :  77.02589988708496 val_loss :  81.63859462738037\n",
      "iteration :  262 loss :  76.94117164611816 val_loss :  81.56246662139893\n",
      "iteration :  263 loss :  76.85986518859863 val_loss :  81.46467876434326\n",
      "iteration :  264 loss :  76.77112579345703 val_loss :  81.3790636062622\n",
      "iteration :  265 loss :  76.69807624816895 val_loss :  81.29839324951172\n",
      "iteration :  266 loss :  76.60119247436523 val_loss :  81.19278335571289\n",
      "iteration :  267 loss :  76.51180458068848 val_loss :  81.09270095825195\n",
      "iteration :  268 loss :  76.41350936889648 val_loss :  80.99216556549072\n",
      "iteration :  269 loss :  76.29252052307129 val_loss :  80.88339233398438\n",
      "iteration :  270 loss :  76.20176315307617 val_loss :  80.7831802368164\n",
      "iteration :  271 loss :  76.07403945922852 val_loss :  80.67347526550293\n",
      "iteration :  272 loss :  75.95515823364258 val_loss :  80.57142734527588\n",
      "iteration :  273 loss :  75.83529090881348 val_loss :  80.45237350463867\n",
      "iteration :  274 loss :  75.69816589355469 val_loss :  80.31657218933105\n",
      "iteration :  275 loss :  75.56009864807129 val_loss :  80.19963645935059\n",
      "iteration :  276 loss :  75.40206146240234 val_loss :  80.05445289611816\n",
      "iteration :  277 loss :  75.22728157043457 val_loss :  79.88555526733398\n",
      "iteration :  278 loss :  75.0573902130127 val_loss :  79.74556159973145\n",
      "iteration :  279 loss :  74.87406921386719 val_loss :  79.59395790100098\n",
      "iteration :  280 loss :  74.68046569824219 val_loss :  79.41984462738037\n",
      "iteration :  281 loss :  74.46806907653809 val_loss :  79.23413372039795\n",
      "iteration :  282 loss :  74.24518013000488 val_loss :  79.06552696228027\n",
      "iteration :  283 loss :  74.02666091918945 val_loss :  78.90913581848145\n",
      "iteration :  284 loss :  73.77999114990234 val_loss :  78.70450592041016\n",
      "iteration :  285 loss :  73.51688766479492 val_loss :  78.4812183380127\n",
      "iteration :  286 loss :  73.22567558288574 val_loss :  78.27349758148193\n",
      "iteration :  287 loss :  72.91764068603516 val_loss :  78.01735019683838\n",
      "iteration :  288 loss :  72.62760162353516 val_loss :  77.75827980041504\n",
      "iteration :  289 loss :  72.28678512573242 val_loss :  77.47708511352539\n",
      "iteration :  290 loss :  71.9454288482666 val_loss :  77.19246578216553\n",
      "iteration :  291 loss :  71.58281421661377 val_loss :  76.89604759216309\n",
      "iteration :  292 loss :  71.20897197723389 val_loss :  76.59189987182617\n",
      "iteration :  293 loss :  70.82349967956543 val_loss :  76.29883861541748\n",
      "iteration :  294 loss :  70.4458589553833 val_loss :  75.99789905548096\n",
      "iteration :  295 loss :  70.02644920349121 val_loss :  75.69911766052246\n",
      "iteration :  296 loss :  69.60347366333008 val_loss :  75.3614387512207\n",
      "iteration :  297 loss :  69.17775535583496 val_loss :  75.06241989135742\n",
      "iteration :  298 loss :  68.7210340499878 val_loss :  74.74444198608398\n",
      "iteration :  299 loss :  68.26367855072021 val_loss :  74.41398525238037\n",
      "iteration :  300 loss :  67.79207992553711 val_loss :  74.0708417892456\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    print('iteration : ', i+1 ,'loss : ', DA_L[i], 'val_loss : ', DV_L[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e29cd",
   "metadata": {},
   "source": [
    "question 8)\n",
    "\n",
    "Ce code réalise une 4-cross validation, l'intérêt d'entrainer le modèle à partir de 4 différents ensembles d'entrainement et de le tester sur 4 ensembles de validation différents est qu'on obtient un modèle plus fiable, moins biaisé, dont les prédictions test devraient être meilleures, la plupart du temps, que si on avait entraîné puis validé le modèle sur seulement un ensemble à chaque fois.\n",
    "\n",
    "Globalement on tire les mêmes conclusions de l'analyse qu'à la question 7) , à ceci près que l'erreur de validation devient plus petite à l'époque d'apprentissage 27, et que l'erreur de validation obtenue est plus petite qu'à la question 7), on peut même dire beaucoup plus proche de l'erreur d'entraînement.\n",
    "\n",
    "En effet, question 7), à l'époque d'apprentissage 300 on avait :\n",
    "\n",
    "loss: 73.0716 - val_loss: 106.8089 .\n",
    "\n",
    "Alors qu'ici avec la 4-cross validation on a loss :  67.79207992553711 - val_loss :  74.0708417892456 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4402c1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5252, 404, 1326, 102)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(DA), np.size(Da), np.size(DT), np.size(Dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6839033f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA.shape, Da.shape, DT.shape, Dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07b14d",
   "metadata": {},
   "source": [
    "question 1)\n",
    "\n",
    "Puisque l'apprentissage se fait selon DA et Da, on en déduit que DA est un tenseur qui représente les données d'entrainement, et que Da représente les valeurs en milliers de dollars qui y sont associées.\n",
    "DA et Da sont ici des array numpy, alors que DA est une array de 404 vecteurs de 13 éléments Da est une array de 404 valeurs flottantes, ce qui explique que la longueur de DA soit égale à 13 fois la longueur de Da, et qui correspond aux 13 caractéristiques d'une maison définissant un prix en milliers de dollars.\n",
    "On retrouve la même relation entre la longueur de DT et celle de Dt, puisque DT est le tenseur qui représente les données de test, et que Dt représente les valeurs en milliers de dollars qui y sont associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a74e3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "(I,F),(B,E)=boston_housing.load_data()\n",
    "m=I.mean(axis=0); s=I.std(axis=0); B-=m; B/=s; I-=m; I/=s\n",
    "A=I.shape[0]//4; G=I[A:]; C=F[A:]\n",
    "H=I[:A]; D=F[:A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26dce22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be1e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d121c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
